<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion - Breynald Shelter</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Breynald Shelter"><meta name="msapplication-TileImage" content="https://raw.githubusercontent.com/Breynald/ImageSource/main/blogs/img/blog.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Breynald Shelter"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify"><meta property="og:type" content="blog"><meta property="og:title" content="An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion"><meta property="og:url" content="http://example.com/2024/09/25/An-Image-is-Worth-One-Word/"><meta property="og:site_name" content="Breynald Shelter"><meta property="og:description" content="Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://t.mwm.moe/fj?random=An Image is Worth One Word"><meta property="article:published_time" content="2024-09-25T05:28:37.000Z"><meta property="article:modified_time" content="2024-09-26T14:32:46.367Z"><meta property="article:author" content="Breynald"><meta property="article:tag" content="Diffusion Model"><meta property="article:tag" content="Personalization"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://t.mwm.moe/fj?random=An Image is Worth One Word"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com/2024/09/25/An-Image-is-Worth-One-Word/"},"headline":"An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion","image":[],"datePublished":"2024-09-25T05:28:37.000Z","dateModified":"2024-09-26T14:32:46.367Z","author":{"@type":"Person","name":"Breynald"},"publisher":{"@type":"Organization","name":"Breynald Shelter","logo":{"@type":"ImageObject","url":"https://raw.githubusercontent.com/Breynald/ImageSource/main/blogs/img/logo.svg"}},"description":"Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify"}</script><link rel="canonical" href="http://example.com/2024/09/25/An-Image-is-Worth-One-Word/"><link rel="icon" href="https://raw.githubusercontent.com/Breynald/ImageSource/main/blogs/img/blog.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://raw.githubusercontent.com/Breynald/ImageSource/main/blogs/img/logo.svg" alt="Breynald Shelter" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a></div><div class="navbar-end"><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="https://t.mwm.moe/fj?random=An Image is Worth One Word" alt="An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-09-25T05:28:37.000Z" title="2024/9/25 13:28:37">2024-09-25</time></span><span class="level-item"><a class="link-muted" href="/categories/Paper/">Paper</a><span> / </span><a class="link-muted" href="/categories/Paper/Diffusion-Model/">Diffusion Model</a></span><span class="level-item">8 minutes read (About 1150 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion</h1><div class="content"><p>Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy?</p>
<span id="more"></span>

<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>This paper propose to overcome these challenges by finding new words in the textual embedding space of pre-trained text-to-image models. We consider the first stage of the text encoding process. Here, an input string is first converted to a set of tokens. Each token is then replaced with its own embedding vector, and these vectors are fed through the downstream model. The goal is to find new embedding vectors that represent new, specific concepts.</p>
<img src="https://raw.githubusercontent.com/Breynald/ImageSource/main/blogs/An%20Image%20is%20Worth%20One%20Word/Figure%201.png" alt="" style="zoom:80%;" />

<p>In summary, our contributions are as follows:</p>
<ul>
<li>We introduce the task of personalized text-to-image generation, where we synthesize novel scenes of user-provided concepts guided by natural language instruction.</li>
<li>We present the idea of “Textual Inversions” in the context of generative models. Here the goal is to find new pseudo-words in the embedding space of a text encoder that can capture both high-level semantics and fine visual details.</li>
<li>We analyze the embedding space in light of GAN-inspired inversion techniques and demonstrate that it also exhibits a tradeoff between distortion and editability. We show that our approach resides on an appealing point on the tradeoff curve.</li>
<li>We evaluate our method against images generated using user-provided captions of the concepts and demonstrate that our embeddings provide higher visual fidelity, and also enable more robust editing.</li>
</ul>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>Our goal is to find pseudo-words that can guide generation, which is a visual task. As such, we propose to find them through a visual reconstruction objective.</p>
<img src="https://raw.githubusercontent.com/Breynald/ImageSource/main/blogs/An%20Image%20is%20Worth%20One%20Word/Figure%202.png" alt="" style="zoom:80%;" />

<p>Typical text encoder models, such as BERT, begin with a text processing step. These embedding vectors are typically learned as part of the text encoder $c_θ$. In our work, we choose this embedding space as the target for inversion. Specifically, we designate a placeholder string,$S_∗$, to represent the new concept we wish to learn.</p>
<p>To find these new embeddings, we use a small set of images (typically 3-5), which depicts our target concept across multiple settings such as varied backgrounds or poses. We find $v_∗$ through direct optimization, by minimizing the LDM loss over images sampled from the small set. To condition the generation, we randomly sample neutral context texts, derived from the CLIP ImageNet templates (Radford et al., 2021). These contain prompts of the form “A photo of $S_∗$”, “A rendition of $S_∗$”, etc. The full list of templates is provided in the supplementary materials.</p>
<h2 id="Qualitative-comparisons-and-applications"><a href="#Qualitative-comparisons-and-applications" class="headerlink" title="Qualitative comparisons and applications"></a>Qualitative comparisons and applications</h2><h3 id="Image-variations"><a href="#Image-variations" class="headerlink" title="Image variations"></a>Image variations</h3><p>In contrast, our method can successfully capture these finer details, and it does so using only a single word embedding. However, note that while our creations are more similar to the source objects, they are still variations that may differ from the source.</p>
<img src="https://raw.githubusercontent.com/Breynald/ImageSource/main/blogs/An%20Image%20is%20Worth%20One%20Word/Figure%203.png" alt="" style="zoom: 80%;" />

<h3 id="Text-guided-synthesis"><a href="#Text-guided-synthesis" class="headerlink" title="Text-guided synthesis"></a>Text-guided synthesis</h3><p>As our results demonstrate, the frozen text-to-image model is able to jointly reason over both the new concepts and its large body of prior knowledge, bringing them together in a new creation.</p>
<img src="https://raw.githubusercontent.com/Breynald/ImageSource/main/blogs/An%20Image%20is%20Worth%20One%20Word/Figure%204.png" alt=""  />

<img src="https://raw.githubusercontent.com/Breynald/ImageSource/main/blogs/An%20Image%20is%20Worth%20One%20Word/Figure%205.png" alt="" style="zoom: 67%;" />

<h3 id="Style-transfer"><a href="#Style-transfer" class="headerlink" title="Style transfer"></a>Style transfer</h3><p>typical use-case for text-guided synthesis is in artistic circles, where users aim to draw upon the unique style of a specific artist and apply it to new creations. Here, we show that our model can also find pseudowords representing a specific, unknown style.</p>
<img src="https://raw.githubusercontent.com/Breynald/ImageSource/main/blogs/An%20Image%20is%20Worth%20One%20Word/Figure%206.png" alt="" style="zoom: 80%;" />

<h3 id="Concept-compositions"><a href="#Concept-compositions" class="headerlink" title="Concept compositions"></a>Concept compositions</h3><p>In Figure 7 we demonstrate compositional synthesis, where the guiding text contains multiple learned concepts. We observe that the model can concurrently reason over multiple novel pseudo-words at the same time. However, it struggles with relations between them (e.g. it fails to place two concepts side-by-side). We hypothesize that this limitation arises because our training considers only single concept scenes, where the concept is at the core of the image. Training on multi-object scenes may alleviate this shortcoming. However, we leave such investigation to future work.</p>
<img src="https://raw.githubusercontent.com/Breynald/ImageSource/main/blogs/An%20Image%20is%20Worth%20One%20Word/Figure%207.png" alt="" style="zoom: 80%;" />

<h3 id="Bias-reduction"><a href="#Bias-reduction" class="headerlink" title="Bias reduction"></a>Bias reduction</h3><p>A common limitation of text-to-image models is that they inherit the biases found in the internet-scale data used to train them. Here, we demonstrate that we can utilize a small, curated dataset in order to learn a new “fairer” word for a biased concept, which can then be used in place of the original to drive a more inclusive generation.</p>
<img src="https://raw.githubusercontent.com/Breynald/ImageSource/main/blogs/An%20Image%20is%20Worth%20One%20Word/Figure%208.png" alt="" style="zoom: 80%;" />

<h3 id="Downstream-applications"><a href="#Downstream-applications" class="headerlink" title="Downstream applications"></a>Downstream applications</h3><p>Finally, we demonstrate that our pseudo-words can be used in downstream models that build on the same initial LDM model. Specifically, we consider the recent Blended Latent Diffusion (Avrahami et al., 2022a) which enables localized text-based editing of images via a mask-based blending process in the latent space of an LDM. In Figure 9 we demonstrate that this localized synthesis process can also be conditioned on our learned pseudo-words, without requiring any additional modifications of the original model.</p>
<img src="https://raw.githubusercontent.com/Breynald/ImageSource/main/blogs/An%20Image%20is%20Worth%20One%20Word/Figure%209.png" alt="" style="zoom:80%;" />

<h2 id="Quantitative-analysis"><a href="#Quantitative-analysis" class="headerlink" title="Quantitative analysis"></a>Quantitative analysis</h2><h3 id="Evaluation-metrics"><a href="#Evaluation-metrics" class="headerlink" title="Evaluation metrics"></a>Evaluation metrics</h3><p>To analyze the quality of latent space embeddings, we consider two fronts: reconstruction and editability.</p>
<ol>
<li>As our method produces variations on the concept and not a specific image, we measure similarity by considering semantic CLIP-space distances. Specifically, for each concept, we generate a 64 of images using the prompt: “A photo of $S_∗$”. Our reconstruction score is then the average pair-wise CLIP-space cosine-similarity between the generated images and the images of the concept-specific training set.</li>
<li>Second, we want to evaluate our ability to modify the concepts using textual prompts. To this end, we produce a set of images using prompts of varying difficulty and settings. These range from background modifications (“A photo of $S_∗$ on the moon”), to style changes (“An oil painting of $S_∗$”), and a compositional prompt (“Elmo holding a $S_∗$”).</li>
</ol>
<h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><img src="https://raw.githubusercontent.com/Breynald/ImageSource/main/blogs/An%20Image%20is%20Worth%20One%20Word/Figure%2010.png" alt="" style="zoom: 80%;" />

<h2 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h2><p>While our method offers increased freedom, it may still struggle with learning precise shapes, instead incorporating the “semantic” essence of a concept. For artistic creations, this is often enough. In the future, we hope to achieve better control over the accuracy of the reconstructed concepts, enabling users to leverage our method for tasks that require greater precision.  </p>
<p>Another limitation of our approach is in the lengthy optimization times. Using our setup, learning a single concept requires roughly two hours. These times could likely be shortened by training an encoder to directly map a set of images to their textual embedding. We aim to explore this line of work in the future.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1]: Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022.</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion</p><p><a href="http://example.com/2024/09/25/An-Image-is-Worth-One-Word/">http://example.com/2024/09/25/An-Image-is-Worth-One-Word/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Breynald</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2024-09-25</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2024-09-26</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Diffusion-Model/">Diffusion Model</a><a class="link-muted mr-2" rel="tag" href="/tags/Personalization/">Personalization</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2024/09/26/DreamBooth/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2024/09/24/Diffusion-Model-Survey/"><span class="level-item">Diffusion Models: A Comprehensive Survey of Methods and Applications</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><script src="https://utteranc.es/client.js" repo="Breynald/Comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://raw.githubusercontent.com/Breynald/ImageSource/main/blogs/img/avatar.jpg" alt="Breynald"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Breynald</p><p class="is-size-6 is-block">Zhejiang University</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Hangzhou China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">6</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">5</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">6</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Breynald" target="_blank" rel="me noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Github" href="https://github.com/Breynald"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Email" href="mailto:breynald@zju.edu.cn"><i class="fas fa-envelope"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Introduction"><span class="level-left"><span class="level-item">1</span><span class="level-item">Introduction</span></span></a></li><li><a class="level is-mobile" href="#Method"><span class="level-left"><span class="level-item">2</span><span class="level-item">Method</span></span></a></li><li><a class="level is-mobile" href="#Qualitative-comparisons-and-applications"><span class="level-left"><span class="level-item">3</span><span class="level-item">Qualitative comparisons and applications</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Image-variations"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">Image variations</span></span></a></li><li><a class="level is-mobile" href="#Text-guided-synthesis"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">Text-guided synthesis</span></span></a></li><li><a class="level is-mobile" href="#Style-transfer"><span class="level-left"><span class="level-item">3.3</span><span class="level-item">Style transfer</span></span></a></li><li><a class="level is-mobile" href="#Concept-compositions"><span class="level-left"><span class="level-item">3.4</span><span class="level-item">Concept compositions</span></span></a></li><li><a class="level is-mobile" href="#Bias-reduction"><span class="level-left"><span class="level-item">3.5</span><span class="level-item">Bias reduction</span></span></a></li><li><a class="level is-mobile" href="#Downstream-applications"><span class="level-left"><span class="level-item">3.6</span><span class="level-item">Downstream applications</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Quantitative-analysis"><span class="level-left"><span class="level-item">4</span><span class="level-item">Quantitative analysis</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Evaluation-metrics"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">Evaluation metrics</span></span></a></li><li><a class="level is-mobile" href="#Results"><span class="level-left"><span class="level-item">4.2</span><span class="level-item">Results</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Limitations"><span class="level-left"><span class="level-item">5</span><span class="level-item">Limitations</span></span></a></li><li><a class="level is-mobile" href="#References"><span class="level-left"><span class="level-item">6</span><span class="level-item">References</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Bugs/"><span class="level-start"><span class="level-item">Bugs</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Code-Piece/"><span class="level-start"><span class="level-item">Code Piece</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Environment-Configuration/"><span class="level-start"><span class="level-item">Environment Configuration</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Paper/"><span class="level-start"><span class="level-item">Paper</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/Paper/Diffusion-Model/"><span class="level-start"><span class="level-item">Diffusion Model</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><figure class="media-left"><a class="image" href="/2024/09/27/MathJax-fails-to-render-in-hexo-icarus/"><img src="https://t.mwm.moe/fj?random=MathJax fails to render in hexo icarus" alt="MathJax fails to render in hexo icarus"></a></figure><div class="media-content"><p class="date"><time dateTime="2024-09-27T01:08:11.000Z">2024-09-27</time></p><p class="title"><a href="/2024/09/27/MathJax-fails-to-render-in-hexo-icarus/">MathJax fails to render in hexo icarus</a></p><p class="categories"><a href="/categories/Bugs/">Bugs</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2024/09/26/DreamBooth/"><img src="https://t.mwm.moe/fj?random=DreamBooth" alt="DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation"></a></figure><div class="media-content"><p class="date"><time dateTime="2024-09-26T14:14:57.000Z">2024-09-26</time></p><p class="title"><a href="/2024/09/26/DreamBooth/">DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</a></p><p class="categories"><a href="/categories/Paper/">Paper</a> / <a href="/categories/Paper/Diffusion-Model/">Diffusion Model</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2024/09/25/An-Image-is-Worth-One-Word/"><img src="https://t.mwm.moe/fj?random=An Image is Worth One Word" alt="An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion"></a></figure><div class="media-content"><p class="date"><time dateTime="2024-09-25T05:28:37.000Z">2024-09-25</time></p><p class="title"><a href="/2024/09/25/An-Image-is-Worth-One-Word/">An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion</a></p><p class="categories"><a href="/categories/Paper/">Paper</a> / <a href="/categories/Paper/Diffusion-Model/">Diffusion Model</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2024/09/24/Diffusion-Model-Survey/"><img src="https://t.mwm.moe/fj?random=Diffusion-Models-Survey" alt="Diffusion Models: A Comprehensive Survey of Methods and Applications"></a></figure><div class="media-content"><p class="date"><time dateTime="2024-09-24T08:03:42.000Z">2024-09-24</time></p><p class="title"><a href="/2024/09/24/Diffusion-Model-Survey/">Diffusion Models: A Comprehensive Survey of Methods and Applications</a></p><p class="categories"><a href="/categories/Paper/">Paper</a> / <a href="/categories/Paper/Diffusion-Model/">Diffusion Model</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2024/09/23/Environment-configuration-for-MagicDrive/"><img src="https://t.mwm.moe/fj?random=Environment-configuration-for-MagicDrive" alt="Environment configuration for MagicDrive"></a></figure><div class="media-content"><p class="date"><time dateTime="2024-09-23T09:05:22.000Z">2024-09-23</time></p><p class="title"><a href="/2024/09/23/Environment-configuration-for-MagicDrive/">Environment configuration for MagicDrive</a></p><p class="categories"><a href="/categories/Environment-Configuration/">Environment Configuration</a></p></div></article></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Bugs/"><span class="tag">Bugs</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Diffusion-Model/"><span class="tag">Diffusion Model</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Driving-Video-Deneration/"><span class="tag">Driving Video Deneration</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/HuggingFace/"><span class="tag">HuggingFace</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Personalization/"><span class="tag">Personalization</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Survey/"><span class="tag">Survey</span><span class="tag">1</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://raw.githubusercontent.com/Breynald/ImageSource/main/blogs/img/logo.svg" alt="Breynald Shelter" height="28"></a><p class="is-size-7"><span>&copy; 2024 Breynald</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="GitHub" href="https://github.com/Breynald"><i class="fab fa-github"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Email" href="mailto:breynald@zju.edu.cn"><i class="fas fa-envelope"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script>if (false == true) {
			$(document).ready(function () {
				var int = setInterval(fixCount, 100);
				var uvOffSet = parseInt(0);
				var pvOffSet = parseInt(0);
				
				function fixCount() {
					var realUv = parseInt($("#busuanzi_value_site_uv").html())
					var realPv = parseInt($("#busuanzi_value_site_pv").html())
					if ($("#busuanzi_container_site_uv").css("display") != "none" && realUv > 0) {
						clearInterval(int);
						$("#busuanzi_value_site_uv").html(realUv + uvOffSet);
						$("#busuanzi_value_site_pv").html(realPv + pvOffSet);
					}
				}
			});
		}</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="/js/pjax.js"></script><!--!--><!--!--><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>