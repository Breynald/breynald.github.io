{"posts":[{"title":"An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion","text":"Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? IntroductionThis paper propose to overcome these challenges by finding new words in the textual embedding space of pre-trained text-to-image models. We consider the first stage of the text encoding process. Here, an input string is first converted to a set of tokens. Each token is then replaced with its own embedding vector, and these vectors are fed through the downstream model. The goal is to find new embedding vectors that represent new, specific concepts. In summary, our contributions are as follows: We introduce the task of personalized text-to-image generation, where we synthesize novel scenes of user-provided concepts guided by natural language instruction. We present the idea of “Textual Inversions” in the context of generative models. Here the goal is to find new pseudo-words in the embedding space of a text encoder that can capture both high-level semantics and fine visual details. We analyze the embedding space in light of GAN-inspired inversion techniques and demonstrate that it also exhibits a tradeoff between distortion and editability. We show that our approach resides on an appealing point on the tradeoff curve. We evaluate our method against images generated using user-provided captions of the concepts and demonstrate that our embeddings provide higher visual fidelity, and also enable more robust editing. MethodOur goal is to find pseudo-words that can guide generation, which is a visual task. As such, we propose to find them through a visual reconstruction objective. Typical text encoder models, such as BERT, begin with a text processing step. These embedding vectors are typically learned as part of the text encoder $c_θ$. In our work, we choose this embedding space as the target for inversion. Specifically, we designate a placeholder string,$S_∗$, to represent the new concept we wish to learn. To find these new embeddings, we use a small set of images (typically 3-5), which depicts our target concept across multiple settings such as varied backgrounds or poses. We find $v_∗$ through direct optimization, by minimizing the LDM loss over images sampled from the small set. To condition the generation, we randomly sample neutral context texts, derived from the CLIP ImageNet templates (Radford et al., 2021). These contain prompts of the form “A photo of $S_∗$”, “A rendition of $S_∗$”, etc. The full list of templates is provided in the supplementary materials. Qualitative comparisons and applicationsImage variationsIn contrast, our method can successfully capture these finer details, and it does so using only a single word embedding. However, note that while our creations are more similar to the source objects, they are still variations that may differ from the source. Text-guided synthesisAs our results demonstrate, the frozen text-to-image model is able to jointly reason over both the new concepts and its large body of prior knowledge, bringing them together in a new creation. Style transfertypical use-case for text-guided synthesis is in artistic circles, where users aim to draw upon the unique style of a specific artist and apply it to new creations. Here, we show that our model can also find pseudowords representing a specific, unknown style. Concept compositionsIn Figure 7 we demonstrate compositional synthesis, where the guiding text contains multiple learned concepts. We observe that the model can concurrently reason over multiple novel pseudo-words at the same time. However, it struggles with relations between them (e.g. it fails to place two concepts side-by-side). We hypothesize that this limitation arises because our training considers only single concept scenes, where the concept is at the core of the image. Training on multi-object scenes may alleviate this shortcoming. However, we leave such investigation to future work. Bias reductionA common limitation of text-to-image models is that they inherit the biases found in the internet-scale data used to train them. Here, we demonstrate that we can utilize a small, curated dataset in order to learn a new “fairer” word for a biased concept, which can then be used in place of the original to drive a more inclusive generation. Downstream applicationsFinally, we demonstrate that our pseudo-words can be used in downstream models that build on the same initial LDM model. Specifically, we consider the recent Blended Latent Diffusion (Avrahami et al., 2022a) which enables localized text-based editing of images via a mask-based blending process in the latent space of an LDM. In Figure 9 we demonstrate that this localized synthesis process can also be conditioned on our learned pseudo-words, without requiring any additional modifications of the original model. Quantitative analysisEvaluation metricsTo analyze the quality of latent space embeddings, we consider two fronts: reconstruction and editability. As our method produces variations on the concept and not a specific image, we measure similarity by considering semantic CLIP-space distances. Specifically, for each concept, we generate a 64 of images using the prompt: “A photo of $S_∗$”. Our reconstruction score is then the average pair-wise CLIP-space cosine-similarity between the generated images and the images of the concept-specific training set. Second, we want to evaluate our ability to modify the concepts using textual prompts. To this end, we produce a set of images using prompts of varying difficulty and settings. These range from background modifications (“A photo of $S_∗$ on the moon”), to style changes (“An oil painting of $S_∗$”), and a compositional prompt (“Elmo holding a $S_∗$”). Results LimitationsWhile our method offers increased freedom, it may still struggle with learning precise shapes, instead incorporating the “semantic” essence of a concept. For artistic creations, this is often enough. In the future, we hope to achieve better control over the accuracy of the reconstructed concepts, enabling users to leverage our method for tasks that require greater precision. Another limitation of our approach is in the lengthy optimization times. Using our setup, learning a single concept requires roughly two hours. These times could likely be shortened by training an encoder to directly map a set of images to their textual embedding. We aim to explore this line of work in the future. References[1]: Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022.","link":"/2024/09/25/An-Image-is-Worth-One-Word/"},{"title":"Diffusion Models: A Comprehensive Survey of Methods and Applications","text":"Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. FOUNDATIONS OF DIFFUSION MODELSDiffusion models are a family of probabilistic generative models that progressively destruct data by injecting noise, then learn to reverse this process for sample generation. Current research on diffusion models is mostly based on three predominant formulations: denoising diffusion probabilistic models (DDPMs), score-based generative models (SGMs), and stochastic differential equations (Score SDEs). Denoising Diffusion Probabilistic Models (DDPMs)A denoising diffusion probabilistic model (DDPM) makes use of two Markov chains: a forward chain that perturbs data to noise, and a reverse chain that converts noise back to data. Formally, given a data distribution $x_0 \\sim q(x_0)$, the forward Markov process generates a sequence of random variables $x_1$, $x_2$ . . . $x_T$ with transition kernel $q(x_t \\mid x_{t-1})$. The joint distribution of $x_1$, $x_2$ . . . $x_T$ conditioned on $x_0$, denoted as $q(x_1, . . . , x_T \\mid x_0)$: $$q(x_1, . . . , x_T \\mid x_0) = \\prod_{i=1}^T q(x_t \\mid x_{t-1}) \\tag{1.1}$$One typical design for the transition kernel is Gaussian perturbation, and the most common choice for the transition kernel is$$q(x_t \\mid x_{t−1}) = \\mathcal{N} (x_t ; \\sqrt{1 − \\beta_t} x_{t−1} , \\beta_t I) \\tag{1.2}$$This Gaussian transition kernel allows us to marginalize the joint distribution in Eq. (1.1) to obtain the analytical form of $q(x_t \\mid x_0)$ for all $t \\in {0, 1, · · · ,T }$. Specifically, with $\\alpha_t := 1 − βt$ and $\\overline\\alpha_t := \\prod_{s=0}^t \\alpha_s $, we have$$q(x_t \\mid x_0) = \\mathcal{N}(x_t; \\sqrt{\\overline\\alpha_t} x_0, (1-\\overline\\alpha_t)I) \\tag {1.3}$$Given $x_0$, we can easily obtain a sample of $x_t$ by sampling a Gaussian vector $\\epsilon \\sim \\mathcal{N} (0, I)$ and applying the transformation$$x_t = \\sqrt{\\overline\\alpha_t}x_0 + \\sqrt{1-\\overline\\alpha_t}\\epsilon \\tag{1.4}$$The learnable transition kernel $p_{\\theta} (x_{t−1} \\mid x_t )$ takes the form of$$p_\\theta(x_{t-1} \\mid x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)) \\tag{1.5}$$where $\\theta$ denotes model parameters, and the mean $ \\mu_\\theta(x_t , t)$ and variance $\\Sigma_\\theta(x_t , t)$ are parameterized by deep neural networks. Key to the success of this sampling process is training the reverse Markov chain to match the actual time reversal of the forward Markov chain. This is achieved by minimizing the Kullback-Leibler (KL) divergence. Ho et al. (2020) propose to reweight various terms in $L_{VLB}$ for better sample quality and noticed an important equivalence between the resulting loss function and the training objective for noise-conditional score networks (NCSNs), one type of score-based generative models, in Song and Ermon. The loss takes the form of$$\\mathbb{E}_{t\\sim\\mathcal{U}[1, T], x_0\\sim q(x_0), \\epsilon\\sim \\mathcal{N}(0, I)}\\left[\\lambda(t)\\Vert\\epsilon - \\epsilon_\\theta(x_t, t)\\Vert^2\\right] \\tag{1.6}$$where $\\lambda(t)$ is a positive weighting function, $x_t$ is computed from $x_0$ and $\\epsilon$ by Eq. (1.4), $\\mathcal{U}[1, T]$ s a uniform distribution over the set ${1, 2, · · · ,T }$, and $\\epsilon_\\theta$ is a deep neural network with parameter $\\theta$ that predicts the noise vector $\\epsilon$ given $x_t$ and $t$. Score-Based Generative Models (SGMs)The key idea of score-based generative models (SGMs) is to perturb data with a sequence of intensifying Gaussian noise and jointly estimate the score functions for all noisy data distributions by training a deep neural network model conditioned on noise levels (called a noise-conditional score network, NCSN). With similar notations in DDPMs, we let $q(x_0)$ be the data distribution, and $0 &lt; \\sigma_1 &lt; \\sigma_2 &lt; · · · &lt; \\sigma_t &lt; · · · &lt; \\sigma_T$ be a sequence of noise levels. A typical example of SGMs involves perturbing a data point $x_0$ to $x_t$ by the Gaussian noise distribution $q(x_t \\mid x_0) = \\mathcal{N} (x_t ; x_0, \\sigma_t^2 I )$. This yields a sequence of noisy data densities $q(x_1), q(x_2), · · · , q(x_T )$. With denoising score matching and similar notations in Eq. (1.6), the training objective is given by$$\\mathbb{E}_{t\\sim\\mathcal{U}\\text{〚}1, T \\text{〛}, x_0\\sim q(x_0), x_t\\sim q(x_t\\mid x_0)}\\left[\\lambda(t)\\sigma_t^2\\Vert \\nabla_{x_t}\\log {q(x_t)} - s_\\theta(x_t, t) \\Vert^2 \\right] \\tag{1.7}$$$$= \\mathbb{E}_{t\\sim\\mathcal{U}\\text{〚}1, T \\text{〛}, x_0\\sim q(x_0), x_t\\sim q(x_t\\mid x_0)}\\left[\\lambda(t)\\sigma_t^2\\Vert \\nabla_{x_t}\\log {q(x_t|x_0)} - s_\\theta(x_t, t) \\Vert^2 \\right] + const \\tag{1.8}$$$$= \\mathbb{E}_{t\\sim\\mathcal{U}\\text{〚}1, T \\text{〛}, x_0\\sim q(x_0), x_t\\sim q(x_t\\mid x_0)}\\left[\\lambda(t)\\Vert -\\frac{x_t - x_0}{\\sigma_t} - \\sigma_ts_\\theta(x_t, t) \\Vert^2 \\right] + const \\tag{1.9}$$$$= \\mathbb{E}_{t\\sim\\mathcal{U}\\text{〚}1, T \\text{〛}, x_0\\sim q(x_0), \\epsilon\\sim \\mathcal{N}(0, I)}\\left[\\lambda(t)\\Vert \\epsilon + \\sigma_ts_\\theta(x_t, t) \\Vert^2 \\right] + const \\tag{1.10}$$ Stochastic Differential Equations (Score SDEs)DDPMs and SGMs can be further generalized to the case of infinite time steps or noise levels, where the perturbation and denoising processes are solutions to stochastic differential equations (SDEs). We call this formulation Score SDE, as it leverages SDEs for noise perturbation and sample generation, and the denoising process requires estimating score functions of noisy data distributions. Score SDEs perturb data to noise with a diffusion process governed by the following stochastic differential equation (SDE):$$dx = f(x, t)dt + g(t)dw \\tag{1.11}$$where $f (x, t) $and $g(t)$ are diffusion and drift functions of the SDE, and w is a standard Wiener process (a.k.a., Brownian motion). The forward processes in DDPMs and SGMs are both discretizations of this SDE. As demonstrated in Song et al. (2020), for DDPMs, the corresponding SDE is:$$dx = -\\frac{1}{2}\\beta(t)xdt + \\sqrt{\\beta(t)}dw \\tag{1.12}$$where $\\beta(\\frac{t}{T}) = T\\beta_t$ as $T$ goes to infinity; and for SGMs, the corresponding SDE is given by$$dx = \\sqrt{\\frac{d[\\sigma(t)^2]}{dt}}dw \\tag{1.13}$$where $\\sigma(\\frac{t}{T}) = \\sigma_t$ as $T$ goes to infinity. Here we use $q_t (x)$ to denote the distribution of $x_t$ in the forward process. Crucially, for any diffusion process in the form of Eq. (1.9), Anderson shows that it can be reversed by solving the following reverse-time SDE:$$dx = \\left[f(x, t) - g(t)^2\\nabla_x\\log{q_t(x)}\\right]dt+g(t)d\\overline{w} \\tag{1.14}$$where $\\overline{w}$ is a standard Wiener process when time flows backwards, and $dt$ denotes an infinitesimal negative time step. Moreover, Song et al. (2020) prove the existence of an ordinary differential equation (ODE), namely the probability flow ODE, whose trajectories have the same marginals as the reverse-time SDE. The probability flow ODE is given by:$$dx = \\left[ f(x, t) - \\frac{1}{2}g(t)^2\\nabla_x\\log{q_t(x)} \\right]dt \\tag{1.15}$$Both the reverse-time SDE and the probability flow ODE allow sampling from the same data distribution as their trajectories have the same marginals. Like in SGMs, we parameterize a time-dependent score model $s_θ (x_t , t)$ to estimate the score function by generalizing the score matching objective to continuous time, leading to the following objective:$$\\mathbb{E}_{t\\sim\\mathcal{U}[0, T], x_0\\sim q(x_0), x_t\\sim q(x_t\\mid x_0)}\\left[\\lambda(t)\\Vert s_\\theta(x_t, t) - \\nabla_{x_t}\\log {q_{0t}(x_t\\mid x_0)\\Vert^2} \\right] \\tag{1.16}$$where $\\mathcal{U}[0, T]$ denotes the uniform distribution over $[0,T ]$. Subsequent research on diffusion models focuses on improving these classical approaches (DDPMs, SGMs, and Score SDEs) from three major directions: faster and more efficient sampling, more accurate likelihood and density estimation, and handling data with special structures (such as permutation invariance, manifold structures, and discrete data).","link":"/2024/09/24/Diffusion-Model-Survey/"},{"title":"HuggingFace Downloader","text":"Code pieces for downloading models from Hugging Face. Install12pip install huggingface_hubhuggingface-cli --help downloader.py123456789101112131415161718192021import osfrom huggingface_hub import snapshot_downloadrepo_id_list=[ # samples &quot;laion/CLIP-ViT-B-16-laion2B-s34B-b88K&quot;, &quot;laion/CLIP-ViT-H-14-laion2B-s32B-b79K&quot;, &quot;openai/clip-vit-base-patch32&quot;]# download to 'Models'for repo_id in repo_id_list: print(&quot;Start Downloading models for repo {}&quot;.format(repo_id)) local_dir = snapshot_download(repo_id=repo_id, local_dir=os.path.join(&quot;../Models&quot;,repo_id) , local_dir_use_symlinks=False, cache_dir=&quot;../.cache&quot;, resume_download=True, endpoint=&quot;https://hf-mirror.com&quot;, use_auth_token=&quot;hf_*************&quot;) print(f&quot;Model file downloaded to {local_dir}&quot;) Run1python downloader.py","link":"/2024/09/22/HuggingFace-Downloader/"},{"title":"DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation","text":"In this work, we present a new approach for “personalization” of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. Introduction A new approach for personalizing large text-to-image diffusion models enables them to generate specific subjects in various contexts. While current models create diverse images from text, they struggle to replicate exact appearances. This method introduces a rare token identifier for each subject, fine-tunes the model with both images and text, and applies a class-specific prior preservation loss to prevent overfitting. This allows the model to synthesize photorealistic images of the subject while maintaining its key features across different scenes (Figure 2). MethodText-to-Image Diffusion ModelsDiffusion models are probabilistic generative models that are trained to learn a data distribution by the gradual denoising of a variable sampled from a Gaussian distribution. They are trained using a squared error loss to denoise a variably-noised image or latent code $z_t := \\alpha_tx + \\sigma_t\\epsilon$ as follows:$$\\mathbb{E}_{x, c ,\\epsilon, t}\\left[ w_t \\Vert \\hat{x}_\\theta(\\alpha_tx + \\sigma_t\\epsilon, c) - x \\Vert^2_2 \\right] \\tag{1}$$details about the loss function referring to Diffusion Models: A Comprehensive Survey of Methods and Applications - Breynald Shelter. Personalization of Text-to-Image ModelsOur first task is to implant the subject instance into the output domain of the model such that we can query the model for varied novel images of the subject. Designing Prompts for Few-Shot PersonalizationIn order to bypass the overhead of writing detailed image descriptions for a given image set we opt for a simpler approach and label all input images of the subject “a [identifier] [class noun]”, where [identifier] is a unique identifier linked to the subject and [class noun] is a coarse class descriptor of the subject (e.g. cat, dog, watch, etc.). In essence, we seek to leverage the model’s prior of the specific class and entangle it with the embedding of our subject’s unique identifier so we can leverage the visual prior to generate new poses and articulations of the subject in different contexts. Rare-token IdentifiersOur approach is to find rare tokens in the vocabulary, and then invert these tokens into text space, in order to minimize the probability of the identifier having a strong prior. We perform a rare-token lookup in the vocabulary and obtain a sequence of rare token identifiers $f (\\hat{V})$, where f is a tokenizer; a function that maps character sequences to tokens and $\\hat{V}$ is the decoded text stemming from the tokens $f (\\hat{V})$. Class-specific Prior Preservation LossTo mitigate the two aforementioned issues during fine-tuning: Model slowly forgets how to generate subjects of the same class as the target subject; The possibility of reduced output diversity. we propose an autogenous class-specific prior preservation loss that encourages diversity and counters language drift. In essence, our method is to supervise the model with its own generated samples, in order for it to retain the prior once the few-shot fine-tuning begins. Specifically, we generate data $x_{pr} = \\hat{x}(z_{t1} , c_{pr})$ by using the ancestral sampler on the frozen pre-trained diffusion model with random initial noise $z_{t1} \\sim \\mathcal{N} (0, I)$ and conditioning vector $c_{pr} := \\Gamma(f (“a [class noun]”))$. The loss becomes:$$\\mathbb{E}_{x, c ,\\epsilon, \\epsilon’, t}\\left[ w_t \\Vert \\hat{x}_\\theta(\\alpha_tx + \\sigma_t\\epsilon, c) - x \\Vert^2_2 \\right] + \\ \\left[ \\lambda w_{t’} \\Vert \\hat{x}_\\theta(\\alpha_{t’}x_{pr} + \\sigma_{t’}\\epsilon’, c_{pr}) - x_{pr} \\Vert^2_2 \\right] \\tag{2}$$where the second term is the prior-preservation term that supervises the model with its own generated images, and $\\lambda$ controls for the relative weight of this term.","link":"/2024/09/26/DreamBooth/"},{"title":"Environment configuration for MagicDrive","text":"A guidance of environment configuration for MagicDrive. Environment SetupPrepare conda environment 12conda create -n newenv python==3.8conda activate newenv Clone MagicDrive reop 1git clone --recursive https://github.com/cure-lab/MagicDrive.git Install pytorch==1.10.2, torchvision==0.11.3, cuda==11.3 12pip install torchvision-0.11.3+cu113-cp38-cp38-linux_x86_64.whlpip install torch-1.10.2+cu113-cp38-cp38-linux_x86_64.whl Prepare mmcv for BEVFusion 1pip install https://download.openmmlab.com/mmcv/dist/cu113/torch1.10.0/mmcv_full-1.4.5-cp38-cp38-manylinux1_x86_64.whl Install other requirements 1pip install -r requirements/dev.txt Install diffusers 12cd third_party/diffuserspip install . Install BEVFusion 1234cd third_partygit clone https://github.com/mit-han-lab/bevfusion.gitcd bevfusionpython setup.py develop Pretrained WeightsThe training is based on stable-diffusion-v1-5. Put them at ${ROOT}/pretrained/ as follows: 123456{ROOT}/pretrained/stable-diffusion-v1-5/├── text_encoder├── tokenizer├── unet├── vae└── ...","link":"/2024/09/23/Environment-configuration-for-MagicDrive/"},{"title":"MathJax fails to render in hexo icarus","text":"In the Hexo Icarus theme, when navigating between pages, MathJax doesn’t render immediately; it only renders after refreshing the page. This issue occurs because MathJax scripts are not being re-initialized on page transitions, which are typically handled by pjax in themes with smooth page transitions. To fix this, you can force MathJax to re-render on every end of the pjax event by using the following approach: 123456// listen pjax:end event in pjax.jsdocument.addEventListener('pjax:end', function () { // MathJax is global, can be used directly MathJax.typesetPromise();});","link":"/2024/09/27/MathJax-fails-to-render-in-hexo-icarus/"}],"tags":[{"name":"Diffusion Model","slug":"Diffusion-Model","link":"/tags/Diffusion-Model/"},{"name":"Personalization","slug":"Personalization","link":"/tags/Personalization/"},{"name":"Survey","slug":"Survey","link":"/tags/Survey/"},{"name":"HuggingFace","slug":"HuggingFace","link":"/tags/HuggingFace/"},{"name":"Driving Video Deneration","slug":"Driving-Video-Deneration","link":"/tags/Driving-Video-Deneration/"},{"name":"Bugs","slug":"Bugs","link":"/tags/Bugs/"}],"categories":[{"name":"Paper","slug":"Paper","link":"/categories/Paper/"},{"name":"Code Piece","slug":"Code-Piece","link":"/categories/Code-Piece/"},{"name":"Diffusion Model","slug":"Paper/Diffusion-Model","link":"/categories/Paper/Diffusion-Model/"},{"name":"Environment Configuration","slug":"Environment-Configuration","link":"/categories/Environment-Configuration/"},{"name":"Bugs","slug":"Bugs","link":"/categories/Bugs/"}],"pages":[]}