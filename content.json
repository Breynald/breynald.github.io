{"posts":[{"title":"Diffusion Models: A Comprehensive Survey of Methods and Applications","text":"Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. FOUNDATIONS OF DIFFUSION MODELSDiffusion models are a family of probabilistic generative models that progressively destruct data by injecting noise, then learn to reverse this process for sample generation. Current research on diffusion models is mostly based on three predominant formulations: denoising diffusion probabilistic models (DDPMs), score-based generative models (SGMs), and stochastic differential equations (Score SDEs). Denoising Diffusion Probabilistic Models (DDPMs)A denoising diffusion probabilistic model (DDPM) makes use of two Markov chains: a forward chain that perturbs data to noise, and a reverse chain that converts noise back to data. Formally, given a data distribution $x_0 \\sim q(x_0)$, the forward Markov process generates a sequence of random variables $x_1$, $x_2$ . . . $x_T$ with transition kernel $q(x_t \\mid x_{t-1})$. The joint distribution of $x_1$, $x_2$ . . . $x_T$ conditioned on $x_0$, denoted as $q(x_1, . . . , x_T \\mid x_0)$: $$q(x_1, . . . , x_T \\mid x_0) = \\prod_{i=1}^T q(x_t \\mid x_{t-1}) \\tag{1.1}$$One typical design for the transition kernel is Gaussian perturbation, and the most common choice for the transition kernel is$$q(x_t \\mid x_{t−1}) = \\mathcal{N} (x_t ; \\sqrt{1 − \\beta_t} x_{t−1} , \\beta_t I) \\tag{1.2}$$This Gaussian transition kernel allows us to marginalize the joint distribution in Eq. (1.1) to obtain the analytical form of $q(x_t \\mid x_0)$ for all $t \\in {0, 1, · · · ,T }$. Specifically, with $\\alpha_t := 1 − βt$ and $\\overline\\alpha_t := \\prod_{s=0}^t \\alpha_s $, we have$$q(x_t \\mid x_0) = \\mathcal{N}(x_t; \\sqrt{\\overline\\alpha_t} x_0, (1-\\overline\\alpha_t)I) \\tag {1.3}$$Given $x_0$, we can easily obtain a sample of $x_t$ by sampling a Gaussian vector $\\epsilon \\sim \\mathcal{N} (0, I)$ and applying the transformation$$x_t = \\sqrt{\\overline\\alpha_t}x_0 + \\sqrt{1-\\overline\\alpha_t}\\epsilon \\tag{1.4}$$The learnable transition kernel $p_{\\theta} (x_{t−1} \\mid x_t )$ takes the form of$$p_\\theta(x_{t-1} \\mid x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)) \\tag{1.5}$$where $\\theta$ denotes model parameters, and the mean $ \\mu_\\theta(x_t , t)$ and variance $\\Sigma_\\theta(x_t , t)$ are parameterized by deep neural networks. Key to the success of this sampling process is training the reverse Markov chain to match the actual time reversal of the forward Markov chain. This is achieved by minimizing the Kullback-Leibler (KL) divergence. Ho et al. (2020) propose to reweight various terms in $L_{VLB}$ for better sample quality and noticed an important equivalence between the resulting loss function and the training objective for noise-conditional score networks (NCSNs), one type of score-based generative models, in Song and Ermon. The loss takes the form of$$\\mathbb{E}_{t\\sim\\mathcal{U}[1, T], x_0\\sim q(x_0), \\epsilon\\sim \\mathcal{N}(0, I)}\\left[\\lambda(t)\\Vert\\epsilon - \\epsilon_\\theta(x_t, t)\\Vert^2\\right] \\tag{1.6}$$where $\\lambda(t)$ is a positive weighting function, $x_t$ is computed from $x_0$ and $\\epsilon$ by Eq. (1.4), $\\mathcal{U}[1, T]$ s a uniform distribution over the set ${1, 2, · · · ,T }$, and $\\epsilon_\\theta$ is a deep neural network with parameter $\\theta$ that predicts the noise vector $\\epsilon$ given $x_t$ and $t$. Score-Based Generative Models (SGMs)The key idea of score-based generative models (SGMs) is to perturb data with a sequence of intensifying Gaussian noise and jointly estimate the score functions for all noisy data distributions by training a deep neural network model conditioned on noise levels (called a noise-conditional score network, NCSN). With similar notations in DDPMs, we let $q(x_0)$ be the data distribution, and $0 &lt; \\sigma_1 &lt; \\sigma_2 &lt; · · · &lt; \\sigma_t &lt; · · · &lt; \\sigma_T$ be a sequence of noise levels. A typical example of SGMs involves perturbing a data point $x_0$ to $x_t$ by the Gaussian noise distribution $q(x_t \\mid x_0) = \\mathcal{N} (x_t ; x_0, \\sigma_t^2 I )$. This yields a sequence of noisy data densities $q(x_1), q(x_2), · · · , q(x_T )$. With denoising score matching and similar notations in Eq. (1.6), the training objective is given by$$\\mathbb{E}_{t\\sim\\mathcal{U}\\text{〚}1, T \\text{〛}, x_0\\sim q(x_0), x_t\\sim q(x_t\\mid x_0)}\\left[\\lambda(t)\\sigma_t^2\\Vert \\nabla_{x_t}\\log {q(x_t)} - s_\\theta(x_t, t) \\Vert^2 \\right] \\tag{1.7}$$$$= \\mathbb{E}_{t\\sim\\mathcal{U}\\text{〚}1, T \\text{〛}, x_0\\sim q(x_0), x_t\\sim q(x_t\\mid x_0)}\\left[\\lambda(t)\\sigma_t^2\\Vert \\nabla_{x_t}\\log {q(x_t|x_0)} - s_\\theta(x_t, t) \\Vert^2 \\right] + const \\tag{1.8}$$$$= \\mathbb{E}_{t\\sim\\mathcal{U}\\text{〚}1, T \\text{〛}, x_0\\sim q(x_0), x_t\\sim q(x_t\\mid x_0)}\\left[\\lambda(t)\\Vert -\\frac{x_t - x_0}{\\sigma_t} - \\sigma_ts_\\theta(x_t, t) \\Vert^2 \\right] + const \\tag{1.9}$$$$= \\mathbb{E}_{t\\sim\\mathcal{U}\\text{〚}1, T \\text{〛}, x_0\\sim q(x_0), \\epsilon\\sim \\mathcal{N}(0, I)}\\left[\\lambda(t)\\Vert \\epsilon + \\sigma_ts_\\theta(x_t, t) \\Vert^2 \\right] + const \\tag{1.10}$$ Stochastic Differential Equations (Score SDEs)DDPMs and SGMs can be further generalized to the case of infinite time steps or noise levels, where the perturbation and denoising processes are solutions to stochastic differential equations (SDEs). We call this formulation Score SDE, as it leverages SDEs for noise perturbation and sample generation, and the denoising process requires estimating score functions of noisy data distributions. Score SDEs perturb data to noise with a diffusion process governed by the following stochastic differential equation (SDE):$$dx = f(x, t)dt + g(t)dw \\tag{1.11}$$where $f (x, t) $and $g(t)$ are diffusion and drift functions of the SDE, and w is a standard Wiener process (a.k.a., Brownian motion). The forward processes in DDPMs and SGMs are both discretizations of this SDE. As demonstrated in Song et al. (2020), for DDPMs, the corresponding SDE is:$$dx = -\\frac{1}{2}\\beta(t)xdt + \\sqrt{\\beta(t)}dw \\tag{1.12}$$where $\\beta(\\frac{t}{T}) = T\\beta_t$ as $T$ goes to infinity; and for SGMs, the corresponding SDE is given by$$dx = \\sqrt{\\frac{d[\\sigma(t)^2]}{dt}}dw \\tag{1.13}$$where $\\sigma(\\frac{t}{T}) = \\sigma_t$ as $T$ goes to infinity. Here we use $q_t (x)$ to denote the distribution of $x_t$ in the forward process. Crucially, for any diffusion process in the form of Eq. (1.9), Anderson shows that it can be reversed by solving the following reverse-time SDE:$$dx = \\left[f(x, t) - g(t)^2\\nabla_x\\log{q_t(x)}\\right]dt+g(t)d\\overline{w} \\tag{1.14}$$where $\\overline{w}$ is a standard Wiener process when time flows backwards, and $dt$ denotes an infinitesimal negative time step. Moreover, Song et al. (2020) prove the existence of an ordinary differential equation (ODE), namely the probability flow ODE, whose trajectories have the same marginals as the reverse-time SDE. The probability flow ODE is given by:$$dx = \\left[ f(x, t) - \\frac{1}{2}g(t)^2\\nabla_x\\log{q_t(x)} \\right]dt \\tag{1.15}$$Both the reverse-time SDE and the probability flow ODE allow sampling from the same data distribution as their trajectories have the same marginals. Like in SGMs, we parameterize a time-dependent score model $s_θ (x_t , t)$ to estimate the score function by generalizing the score matching objective to continuous time, leading to the following objective:$$\\mathbb{E}_{t\\sim\\mathcal{U}[0, T], x_0\\sim q(x_0), x_t\\sim q(x_t\\mid x_0)}\\left[\\lambda(t)\\Vert s_\\theta(x_t, t) - \\nabla_{x_t}\\log {q_{0t}(x_t\\mid x_0)\\Vert^2} \\right] \\tag{1.16}$$where $\\mathcal{U}[0, T]$ denotes the uniform distribution over $[0,T ]$. Subsequent research on diffusion models focuses on improving these classical approaches (DDPMs, SGMs, and Score SDEs) from three major directions: faster and more efficient sampling, more accurate likelihood and density estimation, and handling data with special structures (such as permutation invariance, manifold structures, and discrete data).","link":"/2024/09/24/Diffusion-Model-Survey/"},{"title":"Docker container configuration","text":"A guidance of configuration for Docker container. Install requirementsCheck Nvidia1nvidia-smi Install Docker123sudo apt-get updatesudo apt-get install docker.iodocker --version Install Nvidia Docker Toolkit12sudo apt-get install nvidia-docker2sudo systemctl restart docker Create Docker container support for GPUsLook up in nvidia/cuda Tags | Docker Hub1sudo docker pull nvidia/cuda:12.4.1-base-ubuntu22.04 Create container1docker run -it --gpus all --name my_gpu_container nvidia/cuda:12.4.1-base-ubuntu22.04 /bin/bash Environment configuration in container12apt-get update... OthersStop container1docker stop my_gpu_container Start container1docker start my_gpu_container Enter an active container1docker exec -it my_gpu_container /bin/bash Delete container1docker rm my_gpu_container","link":"/2024/10/01/Docker-container-configuration/"},{"title":"An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion","text":"Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? IntroductionThis paper propose to overcome these challenges by finding new words in the textual embedding space of pre-trained text-to-image models. We consider the first stage of the text encoding process. Here, an input string is first converted to a set of tokens. Each token is then replaced with its own embedding vector, and these vectors are fed through the downstream model. The goal is to find new embedding vectors that represent new, specific concepts. In summary, our contributions are as follows: We introduce the task of personalized text-to-image generation, where we synthesize novel scenes of user-provided concepts guided by natural language instruction. We present the idea of “Textual Inversions” in the context of generative models. Here the goal is to find new pseudo-words in the embedding space of a text encoder that can capture both high-level semantics and fine visual details. We analyze the embedding space in light of GAN-inspired inversion techniques and demonstrate that it also exhibits a tradeoff between distortion and editability. We show that our approach resides on an appealing point on the tradeoff curve. We evaluate our method against images generated using user-provided captions of the concepts and demonstrate that our embeddings provide higher visual fidelity, and also enable more robust editing. MethodOur goal is to find pseudo-words that can guide generation, which is a visual task. As such, we propose to find them through a visual reconstruction objective. Typical text encoder models, such as BERT, begin with a text processing step. These embedding vectors are typically learned as part of the text encoder $c_θ$. In our work, we choose this embedding space as the target for inversion. Specifically, we designate a placeholder string,$S_∗$, to represent the new concept we wish to learn. To find these new embeddings, we use a small set of images (typically 3-5), which depicts our target concept across multiple settings such as varied backgrounds or poses. We find $v_∗$ through direct optimization, by minimizing the LDM loss over images sampled from the small set. To condition the generation, we randomly sample neutral context texts, derived from the CLIP ImageNet templates (Radford et al., 2021). These contain prompts of the form “A photo of $S_∗$”, “A rendition of $S_∗$”, etc. The full list of templates is provided in the supplementary materials. Qualitative comparisons and applicationsImage variationsIn contrast, our method can successfully capture these finer details, and it does so using only a single word embedding. However, note that while our creations are more similar to the source objects, they are still variations that may differ from the source. Text-guided synthesisAs our results demonstrate, the frozen text-to-image model is able to jointly reason over both the new concepts and its large body of prior knowledge, bringing them together in a new creation. Style transfertypical use-case for text-guided synthesis is in artistic circles, where users aim to draw upon the unique style of a specific artist and apply it to new creations. Here, we show that our model can also find pseudowords representing a specific, unknown style. Concept compositionsIn Figure 7 we demonstrate compositional synthesis, where the guiding text contains multiple learned concepts. We observe that the model can concurrently reason over multiple novel pseudo-words at the same time. However, it struggles with relations between them (e.g. it fails to place two concepts side-by-side). We hypothesize that this limitation arises because our training considers only single concept scenes, where the concept is at the core of the image. Training on multi-object scenes may alleviate this shortcoming. However, we leave such investigation to future work. Bias reductionA common limitation of text-to-image models is that they inherit the biases found in the internet-scale data used to train them. Here, we demonstrate that we can utilize a small, curated dataset in order to learn a new “fairer” word for a biased concept, which can then be used in place of the original to drive a more inclusive generation. Downstream applicationsFinally, we demonstrate that our pseudo-words can be used in downstream models that build on the same initial LDM model. Specifically, we consider the recent Blended Latent Diffusion (Avrahami et al., 2022a) which enables localized text-based editing of images via a mask-based blending process in the latent space of an LDM. In Figure 9 we demonstrate that this localized synthesis process can also be conditioned on our learned pseudo-words, without requiring any additional modifications of the original model. Quantitative analysisEvaluation metricsTo analyze the quality of latent space embeddings, we consider two fronts: reconstruction and editability. As our method produces variations on the concept and not a specific image, we measure similarity by considering semantic CLIP-space distances. Specifically, for each concept, we generate a 64 of images using the prompt: “A photo of $S_∗$”. Our reconstruction score is then the average pair-wise CLIP-space cosine-similarity between the generated images and the images of the concept-specific training set. Second, we want to evaluate our ability to modify the concepts using textual prompts. To this end, we produce a set of images using prompts of varying difficulty and settings. These range from background modifications (“A photo of $S_∗$ on the moon”), to style changes (“An oil painting of $S_∗$”), and a compositional prompt (“Elmo holding a $S_∗$”). Results LimitationsWhile our method offers increased freedom, it may still struggle with learning precise shapes, instead incorporating the “semantic” essence of a concept. For artistic creations, this is often enough. In the future, we hope to achieve better control over the accuracy of the reconstructed concepts, enabling users to leverage our method for tasks that require greater precision. Another limitation of our approach is in the lengthy optimization times. Using our setup, learning a single concept requires roughly two hours. These times could likely be shortened by training an encoder to directly map a set of images to their textual embedding. We aim to explore this line of work in the future. References[1]: Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022.","link":"/2024/09/25/An-Image-is-Worth-One-Word/"},{"title":"Environment configuration for MagicDrive","text":"A guidance of environment configuration for MagicDrive. Environment SetupPrepare conda environment 12conda create -n newenv python==3.8conda activate newenv Clone MagicDrive reop 1git clone --recursive https://github.com/cure-lab/MagicDrive.git Install pytorch==1.10.2, torchvision==0.11.3, cuda==11.3 12pip install torchvision-0.11.3+cu113-cp38-cp38-linux_x86_64.whlpip install torch-1.10.2+cu113-cp38-cp38-linux_x86_64.whl Prepare mmcv for BEVFusion 1pip install https://download.openmmlab.com/mmcv/dist/cu113/torch1.10.0/mmcv_full-1.4.5-cp38-cp38-manylinux1_x86_64.whl Install other requirements 1pip install -r requirements/dev.txt Install diffusers 12cd third_party/diffuserspip install . Install BEVFusion 1234cd third_partygit clone https://github.com/mit-han-lab/bevfusion.gitcd bevfusionpython setup.py develop Pretrained WeightsThe training is based on stable-diffusion-v1-5. Put them at ${ROOT}/pretrained/ as follows: 123456{ROOT}/pretrained/stable-diffusion-v1-5/├── text_encoder├── tokenizer├── unet├── vae└── ...","link":"/2024/09/23/Environment-configuration-for-MagicDrive/"},{"title":"DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation","text":"In this work, we present a new approach for “personalization” of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. Introduction A new approach for personalizing large text-to-image diffusion models enables them to generate specific subjects in various contexts. While current models create diverse images from text, they struggle to replicate exact appearances. This method introduces a rare token identifier for each subject, fine-tunes the model with both images and text, and applies a class-specific prior preservation loss to prevent overfitting. This allows the model to synthesize photorealistic images of the subject while maintaining its key features across different scenes (Figure 2). MethodText-to-Image Diffusion ModelsDiffusion models are probabilistic generative models that are trained to learn a data distribution by the gradual denoising of a variable sampled from a Gaussian distribution. They are trained using a squared error loss to denoise a variably-noised image or latent code $z_t := \\alpha_tx + \\sigma_t\\epsilon$ as follows:$$\\mathbb{E}_{x, c ,\\epsilon, t}\\left[ w_t \\Vert \\hat{x}_\\theta(\\alpha_tx + \\sigma_t\\epsilon, c) - x \\Vert^2_2 \\right] \\tag{1}$$details about the loss function referring to Diffusion Models: A Comprehensive Survey of Methods and Applications - Breynald Shelter. Personalization of Text-to-Image ModelsOur first task is to implant the subject instance into the output domain of the model such that we can query the model for varied novel images of the subject. Designing Prompts for Few-Shot PersonalizationIn order to bypass the overhead of writing detailed image descriptions for a given image set we opt for a simpler approach and label all input images of the subject “a [identifier] [class noun]”, where [identifier] is a unique identifier linked to the subject and [class noun] is a coarse class descriptor of the subject (e.g. cat, dog, watch, etc.). In essence, we seek to leverage the model’s prior of the specific class and entangle it with the embedding of our subject’s unique identifier so we can leverage the visual prior to generate new poses and articulations of the subject in different contexts. Rare-token IdentifiersOur approach is to find rare tokens in the vocabulary, and then invert these tokens into text space, in order to minimize the probability of the identifier having a strong prior. We perform a rare-token lookup in the vocabulary and obtain a sequence of rare token identifiers $f (\\hat{V})$, where f is a tokenizer; a function that maps character sequences to tokens and $\\hat{V}$ is the decoded text stemming from the tokens $f (\\hat{V})$. Class-specific Prior Preservation LossTo mitigate the two aforementioned issues during fine-tuning: Model slowly forgets how to generate subjects of the same class as the target subject; The possibility of reduced output diversity. we propose an autogenous class-specific prior preservation loss that encourages diversity and counters language drift. In essence, our method is to supervise the model with its own generated samples, in order for it to retain the prior once the few-shot fine-tuning begins. Specifically, we generate data $x_{pr} = \\hat{x}(z_{t1} , c_{pr})$ by using the ancestral sampler on the frozen pre-trained diffusion model with random initial noise $z_{t1} \\sim \\mathcal{N} (0, I)$ and conditioning vector $c_{pr} := \\Gamma(f (“a [class noun]”))$. The loss becomes:$$\\mathbb{E}_{x, c ,\\epsilon, \\epsilon’, t}\\left[ w_t \\Vert \\hat{x}_\\theta(\\alpha_tx + \\sigma_t\\epsilon, c) - x \\Vert^2_2 \\right] + \\left[ \\lambda w_{t’} \\Vert \\hat{x}_\\theta(\\alpha_{t’}x_{pr} + \\sigma_{t’}\\epsilon’, c_{pr}) - x_{pr} \\Vert^2_2 \\right] \\tag{2}$$where the second term is the prior-preservation term that supervises the model with its own generated images, and $\\lambda$ controls for the relative weight of this term. ExpreimentsComparisons with Textual InversionWe compare our results with Textual Inversion, the recent concurrent work of Gal et al., using the hyperparameters provided in their work. We find that this work is the only comparable work in the literature that is subjectdriven, text-guided and generates novel images. We generate images for DreamBooth using Imagen, DreamBooth using Stable Diffusion and Textual Inversion using Stable Diffusion. We compute DINO and CLIP-I subject fidelity metrics and the CLIP-T prompt fidelity metric. In Table 1 we show sizeable gaps in both subject and prompt fidelity metrics for DreamBooth over Textual Inversion. Further, we compare Textual Inversion (Stable Diffusion) and DreamBooth (Stable Diffusion) by conducting a user study. We average results using majority voting and present them in Table 2. We find an overwhelming preference for DreamBooth for both subject fidelity and prompt fidelity. Prior Preservation Loss AblationThe prior preservation loss seeks to combat language drift and preserve the prior. We compute a prior preservation metric (PRES) by computing the average pairwise DINO embeddings between generated images of random subjects of the prior class and real images of our specific subject. The higher this metric, the more similar random subjects of the class are to our specific subject, indicating collapse of the prior. Additionally, we compute a diversity metric (DIV) using the average LPIPS cosine similarity between generated images of same subject with same prompt. We observe that our model trained with PPL achieves higher diversity (with slightly diminished subject fidelity), which can also be observed qualitatively in Figure 5, where our model trained with PPL overfits less to the environment of the reference images and can generate the dog in more diverse poses and articulations. Class-Prior AblationWe finetune Imagen on a subset of our dataset subjects (5 subjects) with no class noun, a randomly sampled incorrect class noun, and the correct class noun. Subject fidelity results are shown in Table 4, with substantially higher subject fidelity for our proposed approach. ApplicationsRecontextualization Art Renditions &amp; Property Modification LimitationsWe illustrate some failure models of our method in Figure 8. The first is related to not being able to accurately generate the prompted context. Possible reasons are a weak prior for these contexts, or difficulty in generating both the subject and specified concept together due to low probability of co-occurrence in the training set. The second is context-appearance entanglement, where the appearance of the subject changes due to the prompted context, exemplified in Figure 8 with color changes of the backpack. Third, we also observe overfitting to the real images that happen when the prompt is similar to the original setting in which the subject was seen. Other limitations are that some subjects are easier to learn than others (e.g. dogs and cats). Occasionally, with subjects that are rarer, the model is unable to support as many subject variations. Finally, there is also variability in the fidelity of the subject and some generated images might contain hallucinated subject features, depending on the strength of the model prior, and the complexity of the semantic modification. References[1]: Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman. DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 22500-22510","link":"/2024/09/26/DreamBooth/"},{"title":"HuggingFace Downloader","text":"Code pieces for downloading models from Hugging Face. Install12pip install huggingface_hubhuggingface-cli --help downloader.py123456789101112131415161718192021import osfrom huggingface_hub import snapshot_downloadrepo_id_list=[ # samples &quot;laion/CLIP-ViT-B-16-laion2B-s34B-b88K&quot;, &quot;laion/CLIP-ViT-H-14-laion2B-s32B-b79K&quot;, &quot;openai/clip-vit-base-patch32&quot;]# download to 'Models'for repo_id in repo_id_list: print(&quot;Start Downloading models for repo {}&quot;.format(repo_id)) local_dir = snapshot_download(repo_id=repo_id, local_dir=os.path.join(&quot;../Models&quot;,repo_id) , local_dir_use_symlinks=False, cache_dir=&quot;../.cache&quot;, resume_download=True, endpoint=&quot;https://hf-mirror.com&quot;, use_auth_token=&quot;hf_*************&quot;) print(f&quot;Model file downloaded to {local_dir}&quot;) Run1python downloader.py","link":"/2024/09/22/HuggingFace-Downloader/"},{"title":"InstructPix2Pix: Learning to Follow Image Editing Instructions","text":"We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models—a language model (GPT-3) and a text-to-image model (Stable Diffusion)—to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per-example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions. Introductionwe propose an approach for generating a paired dataset that combines multiple large models pretrained on different modalities: a large language model (GPT-3) and a text-to-image model (Stable Diffusion). These two models capture complementary knowledge about language and images that can be combined to create paired training data for a task spanning both modalities. Our model directly performs the image edit in the forward pass, and does not require any additional example images, full descriptions of the input/output images, or per-example finetuning. MethodWe treat instruction-based image editing as a supervised learning problem: first, we generate a paired training dataset of text editing instructions and images before/after the edit; then, we train an image editing diffusion model on this generated dataset Generating a Multi-modal Training DatasetGenerating Instructions and Paired CaptionsWe first operate entirely in the text domain, where we leverage a large language model to take in image captions and produce editing instructions and the resulting text captions after the edit, as shown in Figure 2a. See Table 1a for examples of our written instructions and output captions. Using this data, we fine-tuned the GPT-3 Davinci model for a single epoch using the default training parameters. See Table 1b for example GPT-3 generated data. Our dataset is created by generating a large number of edits and output captions using this trained model, where the input captions are real image captions from LAION-Aesthetics (excluding samples with duplicate captions or duplicate image URLs). Generating Paired Images from Paired CaptionsOne challenge in turning a pair of captions into a pair of corresponding images is that text-to-image models provide no guarantees about image consistency, even under very minor changes of the conditioning prompt. We therefore use Prompt-to-Prompt, a recent method aimed at encouraging multiple generations from a text-to-image diffusion model to be similar. While this greatly helps assimilate generated images, different edits may require different amounts of change in image-space. We therefore generate 100 sample pairs of images per caption-pair, each with a random $p ∼ \\mathcal{U}(0.1, 0.9)$, and filter these samples by using a CLIP-based metric. This metric measures the consistency of the change between the two images (in CLIP space) with the change between the two image captions. InstructPix2PixWe use our generated training data to train a conditional diffusion model that edits images from written instructions. We base our model on Stable Diffusion, a large-scale text-to-image latent diffusion model. Details about training referring to [1]. ResultsWe show instruction-based image editing results on a diverse set of real photographs and artwork, for many edit types and instruction wordings. LimitationsWhile our method is able to produce a wide variety of compelling edits to images, including style, medium, and other contextual changes, there still remain a number of limitations: Our model is limited by the visual quality of the generated dataset, and therefore by the diffusion model used to generate the imagery. Furthermore, our method’s ability to generalize to new edits and make correct associations between visual changes and text instructions is limited by the human-written instructions used to fine-tune GPT-3, by the ability of GPT-3 to create instructions and modify captions, and by the ability of Prompt-to-Prompt to modify generated images. Additionally, we find that performing many sequential edits sometimes causes accumulating artifacts. Furthermore, there are well-documented biases in the data and the pretrained models that our method is based upon. The edited images from our method may inherit these biases or introduce others. References[1]: Tim Brooks, Aleksander Holynski, Alexei A. Efros. InstructPix2Pix: Learning To Follow Image Editing Instructions. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 18392-18402","link":"/2024/09/27/InstructPix2Pix/"},{"title":"Inversion-based Style Transfer with Diffusion Models","text":"Our key idea is to learn the artistic style directly from a single painting and then guide the synthesis without providing complex textual descriptions. Specifically, we perceive style as a learnable textual description of a painting. We propose an inversion-based style transfer method (InST), which can efficiently and accurately learn the key information of an image, thus capturing and transferring the artistic style of a painting. IntroductionIn this paper, we propose a novel example-guided artistic image generation framework (i.e., inversion-based style transfer, InST) which related to style transfer and text-to-image synthesis. Given only a single input painting image, our method can learn and transfer its style to a natural image with a very simple text prompt. To achieve this goal, we need to obtain the representation of the image style, which refers to the set of attributes that appear in the high-level textual description of the image. We define the textual descriptions as “new words” that do not exist in the normal language and obtain the embeddings via inversion method. Specifically, we propose an efficient and accurate textual inversion based on the attention mechanism, which can quickly learn key features from an image, and a stochastic inversion to maintain the semantic of the content image. MethodAs shown in Figure 3, our method involves the pixel, latent, and textual spaces. During training, image $x$ is the same as image $y$. The image embedding of image $x$ is obtained by the CLIP image encoder and then sent to the attention-based inversion module. Textual InversionDetails about textual inversion referring to An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion. However, this optimization-based approach is inefficient, and accurate embeddings are difficult to obtain without overfitting with a single image as training data. We propose a learning method based on multi-layer cross attention. The image encoder $\\tau_θ$ projects y to an image embedding $\\tau_\\theta(y)$. The multi-layer cross attention starts with $v_0 = \\tau_\\theta(y)$. Then each layer implements $Attention$ as follows:$$Q_i = W_Q^{(i)} \\cdot v_i, K = W_K^{(i)} \\cdot \\tau_\\theta(y), V = W_V^{(i)} \\cdot \\tau_\\theta(y) \\tag{1}$$ $$v_{i+1} = Attention (Q_i, K, V) \\tag {2}$$ During training, the model is conditioned by the corresponding text embedding only. To avoid overfitting, we apply a dropout strategy in each cross-attention layer, which is set to 0.05. Our optimization goal can finally be defined as follows:$$\\hat{v} = \\mathop{\\arg\\min}_v \\mathbb{E}_{z, x, y, t}\\left[ \\Vert \\epsilon - \\epsilon_\\theta(z_t, t, MultiAtt(\\tau_\\theta(y))) \\Vert^2_2 \\right] \\tag{3}$$where $z ∼ E(x)$, $\\epsilon ∼ \\mathcal N (0, 1)$. $\\tau_θ$ and $\\epsilon_θ$ are fixed during training. In this manner, $\\hat v$ is efficiently optimized to the target area. Stochastic InversionWe divide pre-trained text-to-image diffusion model-based image representation into two parts: holistic representation and detail representation. Holistic representation involves text conditions, and the detail representation is controlled by random noise. We first add random noise to the content image and then use the denoising U-Net in the diffusion model to predict the noise in the image. The predicted noise is used as the initial input noise during generation to preserve the content. Specifically, for each image $z$, the stochastic inversion module takes the image latent code $z = E(y)$ as input. Then $z_t$, the noisy version of $z$, is set as computable parameters, and $\\epsilon_t$ is obtained as follows:$$\\hat{\\epsilon_t} = (z_{t-1} - \\mu_T(z_t, t))\\sigma_t \\tag{4}$$ ExperimentsComparison with Style Transfer Methods Comparison with Text-Guided Methods Ablation StudyStochastic inversion: The full model can maintain the content information and reduce the impact of the style image’s semantic. Hyper-parameter Strength: The larger the Strength, the stronger the influence of the style image on the generated result, vice versa, the generated image is closer to the content image. Attention module: The multi-layer attention helps the content of the generated image be better controlled by the input text conditions and improves editability. Dropout: By dropping the parameters of the latent embeddings, both the accuracy and the editability are improved. LimitationsAlthough our method can transfer typical colors to some extent, when a significant difference exists between the colors of the content and reference images, our method may fail to semantically transfer the color in a one-to-one correspondence. References[1]: Y. Zhang et al., “Inversion-based Style Transfer with Diffusion Models,” 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Vancouver, BC, Canada, 2023, pp. 10146-10156, doi: 10.1109/CVPR52729.2023.00978.","link":"/2024/09/28/Inversion-based-Style-Transfer-with-Diffusion-Models/"},{"title":"幽默语篇生成与理解的社会心理阐释","text":"幽默既是一种智慧武器，又是政治斗争的有力武器。幽默语篇生成反映了幽默主体优越、敌视和追求权利地位的社会心理。幽默语篇理解的社会心理特点在于心理的突然扑空和意外结局的瞬间醒悟给受众带来的成就感和满足感。幽默效果受到人们的心境、联想能力和参与意识等条件的制约。 幽默语篇生成上的社会心理特征幽默反映人的优越心理人们经常嘲笑别人的弱点和不幸。比如冬天看到有人在大街上人滑倒时旁观者会有不同的反应，但当摔倒的人从地上爬起并不好意思地抖落身上的雪时，大多数旁观者都会笑一笑。这种笑就是对别人不幸的嘲笑。人们都不喜欢受人嘲笑，因为受嘲笑就是受轻视。实验表明，人们对那些自己不喜欢的人发生意外情况时更容易笑起来，这正说明幽默是人优越感心理的表现方式。需要指出的是，开玩笑的人和被嘲笑的人对笑话尤其是侮辱性笑话的态度是截然不同的。被嘲笑的人一般是不笑的。这一点进一步证明了幽默是为社会地位而斗争的特殊武器。 幽默反映人的敌视心理根据心理分析理论，幽默及其所产生的笑是为某一群体的敌视行为服务的，无论是否指向具体的对象，幽默都带有敌视性。Albert Rapp（1951）和他的支持者们认为，笑是憎恨和敌视的产物。敌视是人生来就有的，如果人生来不会敌视，就不会产生笑，也就不会需要幽默。现代幽默的所有类型都保留了敌视性特点。只是一些类型中表现明显，另一些类型中表现比较含蓄。 幽默对人类的重要性还有一点可以证明，那就是很少有人承认自己没有幽默感。每个人都可能承认自己视力不好、不会游泳、不会射击等缺陷，但都不会认为自己没有幽默感。如果我们认为周围的某人没有幽默感，那会被认为是对他致命的蔑视。所以，幽默感被人们本能地认为是自己生活中极为重要的东西，是追求权力和获取资源的能力的表现。 幽默反映人追求权力和社会地位的心理根据 R. Provine 对同一职业、不同资历的人群的调查结果，职位高的人爱开玩笑，在同一时间内讲笑话的数量平均达到 7.5 个，职位中等的人讲笑话的数量平均为 5.5 个，而职位低的人讲笑话的数量仅为 0.7 个（转引自 К.Глинка 2004：33－34）。R. Provine 指出，在一些等级制度分明的国家里这一点表现得更为突出。这说明幽默感是社会地位高低的一种表现形式。在一个交际群体内部地位越高，开玩笑的权利越大、机会越多，地位高的人可以嘲笑比自己地位低的人，反之社会规范则不允许。在民主国家里，幽默是争夺权力的致命武器。 讲笑话人 听笑话人 笑话数量 讲笑话人的笑比例 听笑话人的笑比例 男性 男性 275 75.6% 60.0% 女性 女性 502 86.0% 49.8% 男性 女性 238 66.0% 71.0% 女性 男性 185 88.1% 38.9% 上述调查结果显示，权力意识强的男性对女性所讲的笑话反映不太敏感，笑的比例仅为 38.9%，而女性总体上笑的频率比男性高，而且对男性所讲笑话比自己同性所讲笑话的笑反应次数高出 20 多个百分点。从上表中我们还会发现，讲笑话的人比听笑话的人笑得更频繁。 对此现象 А.Дмитриев(1996:67) 作了这样的解释：任何人在和别人交往时，通常都会力争保持自己的形象，树立自己的威信。在别人面前树立威信是每个人的精神需求，这一需求促使他积极表现。笑话引人发笑，给讲笑话的人带来快感。我们认为，讲笑话的人通过笑话使人发笑提高了自己在别人面前的地位，激发了优越感，甚至每次讲同一个笑话时仍然感到自己高于别人，比别人优先理解了笑话的含意。这说明人在其他人面前视幽默为提升自己社会地位的工具，是实现高于别人的优越感的方式和手段。那么，受众的笑是否也意味着提高社会地位的需求？我们认为，幽默不仅能提升讲笑话人的社会地位，同时也使受众得到心理上的满足。笑的优先反应证明自己比别人聪明，比那些迟钝的人优越。因此，笑反应与人的社会地位提升愿望相关，而幽默是实现这种提升的手段和武器，是人追求地位和权力的心理表现。 幽默语篇理解上的社会心理特点受众的社会心理特点幽默效果不仅取决于幽默的质量和讲笑话的技巧，更取决于受众当时的心境和幽默意外结局的瞬间醒悟给他带来的满足程度。幽默效果是这两项相加的总和。受众的心境决定幽默的效果。人在心情好的时候愿意笑，也容易逗笑。心境也是对幽默意外结局醒悟的前提和基础。如果受众心情不好，再高质量的幽默也不会激起他的兴趣，更谈不上对幽默结局的瞬间醒悟。 正如前文所述，幽默的目的是获得或巩固自己在社会上的某种优越感，也被称为意外胜利感。因此我们认为，幽默效果恰恰是通过人的社会地位的提升所带来的成就感来达到的。成就感来自于别人的赞赏和认可、异性的关注或其他人们所看重的东西。幽默的真正目的就是要获得这种成就感，幽默的效果就在于改变社会地位。幽默的最佳效果不在于笑话创作的精湛，也不在于讲笑话的高超技艺，而在于讲笑话和听笑话的人同时都得到高于旁人的优越感，在于他们在社会、集体、家庭中地位的提升以及对局势的控制所带来的满足感。 在幽默语篇理解的过程中，我们知道了它的开头和情节之后，就会猜测它的结局。大脑中就会形成各种各样的预测结果，在我们听的过程中一直在筛选着预测的答案。由于时间短，备选的结果又多，大脑来不及对数量众多的可能性结果进行评价，也无法快速确定哪些结果更有可能成为答案，因此最后大量的预测结果化整为零，头脑中只剩下一个可能的答案。而幽默的结局却出人意料，使受众头脑中唯一的答案也得到否定。意外结局的效果在于，幽默中所发生的事件没有进入到受众大脑中形成的答案群（К.Глинка 2004：47－48），形成了心理的突然扑空和瞬间醒悟，使幽默产生出人意料的效果。 受众的心理条件受众轻松的心理状态和精神准备正如前文所述，受众的心境决定幽默的效果。幽默要想使人发笑，达到最佳幽默效果需要受众具备一些心理准备。弗洛伊德、达尔文等学者认为，人发笑的前提首先是人轻松愉悦的心理状态和接受幽默的精神准备（К.Глинка 2004：42）。 受众的心理联想和参与意识一般来说，幽默的理解过程需要受众对幽默语篇的主人公进行联想和想象，建立自己和主人公的联系，意念中使自己参与到他的行为中去，设身处地地想象幽默情景，才能深刻理解幽默的含意，取得幽默的效果。受众感到自己高于幽默中的主人公，获得优越感，同时在潜意识中与那些没有参与进来、对幽默含意没有醒悟的其他受众作比较，感到自己高于他们，从而得到了极大的满足感。因此联想和主动参与也是幽默语篇理解的重要前提。 结语幽默既是一种无形的智慧武器，又是政治斗争的有力武器；是幽默主体在群体中提高社会地位、威信、引人注目、获取成就感的有效手段。幽默语篇生成反映了幽默主体优越、敌视和追求权力地位的社会心理。幽默语篇理解的社会心理特点在于心理的突然扑空和意外结局的瞬间醒悟给受众带来的成就感和满足感。幽默效果受到人们的心境、联想能力和参与意识等条件的制约。 参考文献[1]: 王金玲. (2006). 幽默语篇生成与理解的社会心理阐释. 黑龙江大学外国语言文学博士后流动站 . 收稿日期: 2006-06-15. www.sinoss.net/uploadfile/2010/1130/6367.pdf.","link":"/2024/10/12/The-Psychological-Process-of-Humor-Discourse-Comprehension-and-Audience-Response/"},{"title":"MathJax fails to render in hexo icarus","text":"In the Hexo Icarus theme, when navigating between pages, MathJax doesn’t render immediately; it only renders after refreshing the page. This issue occurs because MathJax scripts are not being re-initialized on page transitions, which are typically handled by pjax in themes with smooth page transitions. To fix this, you can force MathJax to re-render on every end of the pjax event by using the following approach: 123456// listen pjax:end event in pjax.jsdocument.addEventListener('pjax:end', function () { // MathJax is global, can be used directly MathJax.typesetPromise();});","link":"/2024/09/27/MathJax-fails-to-render-in-hexo-icarus/"}],"tags":[{"name":"Diffusion Model","slug":"Diffusion-Model","link":"/tags/Diffusion-Model/"},{"name":"Survey","slug":"Survey","link":"/tags/Survey/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"Personalization","slug":"Personalization","link":"/tags/Personalization/"},{"name":"Driving Video Deneration","slug":"Driving-Video-Deneration","link":"/tags/Driving-Video-Deneration/"},{"name":"HuggingFace","slug":"HuggingFace","link":"/tags/HuggingFace/"},{"name":"Humor","slug":"Humor","link":"/tags/Humor/"},{"name":"Bugs","slug":"Bugs","link":"/tags/Bugs/"}],"categories":[{"name":"Paper","slug":"Paper","link":"/categories/Paper/"},{"name":"Environment Configuration","slug":"Environment-Configuration","link":"/categories/Environment-Configuration/"},{"name":"Diffusion Model","slug":"Paper/Diffusion-Model","link":"/categories/Paper/Diffusion-Model/"},{"name":"Code Piece","slug":"Code-Piece","link":"/categories/Code-Piece/"},{"name":"Humor","slug":"Paper/Humor","link":"/categories/Paper/Humor/"},{"name":"Bugs","slug":"Bugs","link":"/categories/Bugs/"}],"pages":[]}