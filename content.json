{"posts":[{"title":"HuggingFace Downloader","text":"Code pieces for downloading models from Hugging Face. Install12pip install huggingface_hubhuggingface-cli --help downloader.py123456789101112131415161718192021import osfrom huggingface_hub import snapshot_downloadrepo_id_list=[ # samples &quot;laion/CLIP-ViT-B-16-laion2B-s34B-b88K&quot;, &quot;laion/CLIP-ViT-H-14-laion2B-s32B-b79K&quot;, &quot;openai/clip-vit-base-patch32&quot;]# download to 'Models'for repo_id in repo_id_list: print(&quot;Start Downloading models for repo {}&quot;.format(repo_id)) local_dir = snapshot_download(repo_id=repo_id, local_dir=os.path.join(&quot;../Models&quot;,repo_id) , local_dir_use_symlinks=False, cache_dir=&quot;../.cache&quot;, resume_download=True, endpoint=&quot;https://hf-mirror.com&quot;, use_auth_token=&quot;hf_*************&quot;) print(f&quot;Model file downloaded to {local_dir}&quot;) Run1python downloader.py","link":"/2024/09/22/HuggingFace-Downloader/"},{"title":"Diffusion Models: A Comprehensive Survey of Methods and Applications","text":"Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. FOUNDATIONS OF DIFFUSION MODELSDiffusion models are a family of probabilistic generative models that progressively destruct data by injecting noise, then learn to reverse this process for sample generation. Current research on diffusion models is mostly based on three predominant formulations: denoising diffusion probabilistic models (DDPMs), score-based generative models (SGMs), and stochastic differential equations (Score SDEs). Denoising Diffusion Probabilistic Models (DDPMs)A denoising diffusion probabilistic model (DDPM) makes use of two Markov chains: a forward chain that perturbs data to noise, and a reverse chain that converts noise back to data. Formally, given a data distribution $x_0 \\sim q(x_0)$, the forward Markov process generates a sequence of random variables $x_1$, $x_2$ . . . $x_T$ with transition kernel $q(x_t \\mid x_{t-1})$. The joint distribution of $x_1$, $x_2$ . . . $x_T$ conditioned on $x_0$, denoted as $q(x_1, . . . , x_T \\mid x_0)$: $$q(x_1, . . . , x_T \\mid x_0) = \\prod_{i=1}^T q(x_t \\mid x_{t-1}) \\tag{1.1} \\label{eq:1.1}$$One typical design for the transition kernel is Gaussian perturbation, and the most common choice for the transition kernel is$$q(x_t \\mid x_{t−1}) = \\mathcal{N} (x_t ; \\sqrt{1 − \\beta_t} x_{t−1} , \\beta_t I) \\tag{1.2}$$This Gaussian transition kernel allows us to marginalize the joint distribution in Eq. (1.1) to obtain the analytical form of $q(x_t \\mid x_0)$ for all $t \\in {0, 1, · · · ,T }$. Specifically, with $\\alpha_t := 1 − βt$ and $\\overline\\alpha_t := \\prod_{s=0}^t \\alpha_s $, we have$$q(x_t \\mid x_0) = \\mathcal{N}(x_t; \\sqrt{\\overline\\alpha_t} x_0, (1-\\overline\\alpha_t)I) \\tag {1.3}$$Given $x_0$, we can easily obtain a sample of $x_t$ by sampling a Gaussian vector $\\epsilon \\sim \\mathcal{N} (0, I)$ and applying the transformation$$x_t = \\sqrt{\\overline\\alpha_t}x_0 + \\sqrt{1-\\overline\\alpha_t}\\epsilon \\tag{1.4}$$The learnable transition kernel $p_{\\theta} (x_{t−1} \\mid x_t )$ takes the form of$$p_\\theta(x_{t-1} \\mid x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)) \\tag{1.5}$$where $\\theta$ denotes model parameters, and the mean $ \\mu_\\theta(x_t , t)$ and variance $\\Sigma_\\theta(x_t , t)$ are parameterized by deep neural networks. Key to the success of this sampling process is training the reverse Markov chain to match the actual time reversal of the forward Markov chain. This is achieved by minimizing the Kullback-Leibler (KL) divergence. Ho et al. (2020) propose to reweight various terms in $L_{VLB}$ for better sample quality and noticed an important equivalence between the resulting loss function and the training objective for noise-conditional score networks (NCSNs), one type of score-based generative models, in Song and Ermon. The loss takes the form of$$\\mathbb{E}_{t\\sim\\mathcal{U}\\text{〚}1, T \\text{〛}, x_0\\sim q(x_0), \\epsilon\\sim \\mathcal{N}(0, I)}[\\lambda(t)\\Vert\\epsilon - \\epsilon_\\theta(x_t, t)\\Vert^2] \\tag{1.6}$$where $\\lambda(t)$ is a positive weighting function, $x_t$ is computed from $x_0$ and $\\epsilon$ by Eq. (1.4), $\\mathcal{U}\\text{〚}1, T \\text{〛}$ s a uniform distribution over the set ${1, 2, · · · ,T }$, and $\\epsilon_\\theta$ is a deep neural network with parameter $\\theta$ that predicts the noise vector $\\epsilon$ given $x_t$ and $t$. Score-Based Generative Models (SGMs)The key idea of score-based generative models (SGMs) is to perturb data with a sequence of intensifying Gaussian noise and jointly estimate the score functions for all noisy data distributions by training a deep neural network model conditioned on noise levels (called a noise-conditional score network, NCSN). Stochastic Differential Equations (Score SDEs)DDPMs and SGMs can be further generalized to the case of infinite time steps or noise levels, where the perturbation and denoising processes are solutions to stochastic differential equations (SDEs). We call this formulation Score SDE, as it leverages SDEs for noise perturbation and sample generation, and the denoising process requires estimating score functions of noisy data distributions. Score SDEs perturb data to noise with a diffusion process governed by the following stochastic differential equation (SDE):$$dx = f(x, t)dt + g(t)dw \\tag{1.7}$$where $f (x, t) $and $g(t)$ are diffusion and drift functions of the SDE, and w is a standard Wiener process (a.k.a., Brownian motion). The forward processes in DDPMs and SGMs are both discretizations of this SDE. As demonstrated in Song et al. (2020), for DDPMs, the corresponding SDE is:$$dx = -\\frac{1}{2}\\beta(t)xdt + \\sqrt{\\beta(t)}dw \\tag{1.8}$$where $\\beta(\\frac{t}{T}) = T\\beta_t$ as $T$ goes to infinity; and for SGMs, the corresponding SDE is given by$$dx = \\sqrt{\\frac{d[\\sigma(t)^2]}{dt}}dw \\tag{1.9}$$where $\\sigma(\\frac{t}{T}) = \\sigma_t$ as $T$ goes to infinity. Here we use $q_t (x)$ to denote the distribution of $x_t$ in the forward process.","link":"/2024/09/24/Diffusion-Model-Survey/"},{"title":"Environment configuration for MagicDrive","text":"A guidance of environment configuration for MagicDrive. Environment SetupPrepare conda environment 12conda create -n newenv python==3.8conda activate newenv Clone MagicDrive reop 1git clone --recursive https://github.com/cure-lab/MagicDrive.git Install pytorch==1.10.2, torchvision==0.11.3, cuda==11.3 12pip install torchvision-0.11.3+cu113-cp38-cp38-linux_x86_64.whlpip install torch-1.10.2+cu113-cp38-cp38-linux_x86_64.whl Prepare mmcv for BEVFusion 1pip install https://download.openmmlab.com/mmcv/dist/cu113/torch1.10.0/mmcv_full-1.4.5-cp38-cp38-manylinux1_x86_64.whl Install other requirements 1pip install -r requirements/dev.txt Install diffusers 12cd third_party/diffuserspip install . Install BEVFusion 1234cd third_partygit clone https://github.com/mit-han-lab/bevfusion.gitcd bevfusionpython setup.py develop Pretrained WeightsThe training is based on stable-diffusion-v1-5. Put them at ${ROOT}/pretrained/ as follows: 123456{ROOT}/pretrained/stable-diffusion-v1-5/├── text_encoder├── tokenizer├── unet├── vae└── ...","link":"/2024/09/23/Environment-configuration-for-MagicDrive/"}],"tags":[{"name":"HuggingFace","slug":"HuggingFace","link":"/tags/HuggingFace/"},{"name":"Diffusion Model","slug":"Diffusion-Model","link":"/tags/Diffusion-Model/"},{"name":"Survey","slug":"Survey","link":"/tags/Survey/"},{"name":"Driving Video Deneration","slug":"Driving-Video-Deneration","link":"/tags/Driving-Video-Deneration/"}],"categories":[{"name":"Code Piece","slug":"Code-Piece","link":"/categories/Code-Piece/"},{"name":"Paper","slug":"Paper","link":"/categories/Paper/"},{"name":"Environment Configuration","slug":"Environment-Configuration","link":"/categories/Environment-Configuration/"},{"name":"Diffusion Model","slug":"Paper/Diffusion-Model","link":"/categories/Paper/Diffusion-Model/"}],"pages":[]}