{"posts":[{"title":"An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion","text":"Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? IntroductionThis paper propose to overcome these challenges by finding new words in the textual embedding space of pre-trained text-to-image models. We consider the first stage of the text encoding process. Here, an input string is first converted to a set of tokens. Each token is then replaced with its own embedding vector, and these vectors are fed through the downstream model. The goal is to find new embedding vectors that represent new, specific concepts. In summary, our contributions are as follows: We introduce the task of personalized text-to-image generation, where we synthesize novel scenes of user-provided concepts guided by natural language instruction. We present the idea of â€œTextual Inversionsâ€ in the context of generative models. Here the goal is to find new pseudo-words in the embedding space of a text encoder that can capture both high-level semantics and fine visual details. We analyze the embedding space in light of GAN-inspired inversion techniques and demonstrate that it also exhibits a tradeoff between distortion and editability. We show that our approach resides on an appealing point on the tradeoff curve. We evaluate our method against images generated using user-provided captions of the concepts and demonstrate that our embeddings provide higher visual fidelity, and also enable more robust editing. MethodOur goal is to find pseudo-words that can guide generation, which is a visual task. As such, we propose to find them through a visual reconstruction objective. Typical text encoder models, such as BERT, begin with a text processing step. These embedding vectors are typically learned as part of the text encoder $c_Î¸$. In our work, we choose this embedding space as the target for inversion. Specifically, we designate a placeholder string,$S_âˆ—$, to represent the new concept we wish to learn. To find these new embeddings, we use a small set of images (typically 3-5), which depicts our target concept across multiple settings such as varied backgrounds or poses. We find $v_âˆ—$ through direct optimization, by minimizing the LDM loss over images sampled from the small set. To condition the generation, we randomly sample neutral context texts, derived from the CLIP ImageNet templates (Radford et al., 2021). These contain prompts of the form â€œA photo of $S_âˆ—$â€, â€œA rendition of $S_âˆ—$â€, etc. The full list of templates is provided in the supplementary materials. Qualitative comparisons and applicationsImage variationsIn contrast, our method can successfully capture these finer details, and it does so using only a single word embedding. However, note that while our creations are more similar to the source objects, they are still variations that may differ from the source. Text-guided synthesisAs our results demonstrate, the frozen text-to-image model is able to jointly reason over both the new concepts and its large body of prior knowledge, bringing them together in a new creation. Style transfertypical use-case for text-guided synthesis is in artistic circles, where users aim to draw upon the unique style of a specific artist and apply it to new creations. Here, we show that our model can also find pseudowords representing a specific, unknown style. Concept compositionsIn Figure 7 we demonstrate compositional synthesis, where the guiding text contains multiple learned concepts. We observe that the model can concurrently reason over multiple novel pseudo-words at the same time. However, it struggles with relations between them (e.g. it fails to place two concepts side-by-side). We hypothesize that this limitation arises because our training considers only single concept scenes, where the concept is at the core of the image. Training on multi-object scenes may alleviate this shortcoming. However, we leave such investigation to future work. Bias reductionA common limitation of text-to-image models is that they inherit the biases found in the internet-scale data used to train them. Here, we demonstrate that we can utilize a small, curated dataset in order to learn a new â€œfairerâ€ word for a biased concept, which can then be used in place of the original to drive a more inclusive generation. Downstream applicationsFinally, we demonstrate that our pseudo-words can be used in downstream models that build on the same initial LDM model. Specifically, we consider the recent Blended Latent Diffusion (Avrahami et al., 2022a) which enables localized text-based editing of images via a mask-based blending process in the latent space of an LDM. In Figure 9 we demonstrate that this localized synthesis process can also be conditioned on our learned pseudo-words, without requiring any additional modifications of the original model. Quantitative analysisEvaluation metricsTo analyze the quality of latent space embeddings, we consider two fronts: reconstruction and editability. As our method produces variations on the concept and not a specific image, we measure similarity by considering semantic CLIP-space distances. Specifically, for each concept, we generate a 64 of images using the prompt: â€œA photo of $S_âˆ—$â€. Our reconstruction score is then the average pair-wise CLIP-space cosine-similarity between the generated images and the images of the concept-specific training set. Second, we want to evaluate our ability to modify the concepts using textual prompts. To this end, we produce a set of images using prompts of varying difficulty and settings. These range from background modifications (â€œA photo of $S_âˆ—$ on the moonâ€), to style changes (â€œAn oil painting of $S_âˆ—$â€), and a compositional prompt (â€œElmo holding a $S_âˆ—$â€). Results LimitationsWhile our method offers increased freedom, it may still struggle with learning precise shapes, instead incorporating the â€œsemanticâ€ essence of a concept. For artistic creations, this is often enough. In the future, we hope to achieve better control over the accuracy of the reconstructed concepts, enabling users to leverage our method for tasks that require greater precision. Another limitation of our approach is in the lengthy optimization times. Using our setup, learning a single concept requires roughly two hours. These times could likely be shortened by training an encoder to directly map a set of images to their textual embedding. We aim to explore this line of work in the future. References[1]: Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022.","link":"/2024/09/25/An-Image-is-Worth-One-Word/"},{"title":"Diffusion Models: A Comprehensive Survey of Methods and Applications","text":"Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. FOUNDATIONS OF DIFFUSION MODELSDiffusion models are a family of probabilistic generative models that progressively destruct data by injecting noise, then learn to reverse this process for sample generation. Current research on diffusion models is mostly based on three predominant formulations: denoising diffusion probabilistic models (DDPMs), score-based generative models (SGMs), and stochastic differential equations (Score SDEs). Denoising Diffusion Probabilistic Models (DDPMs)A denoising diffusion probabilistic model (DDPM) makes use of two Markov chains: a forward chain that perturbs data to noise, and a reverse chain that converts noise back to data. Formally, given a data distribution $x_0 \\sim q(x_0)$, the forward Markov process generates a sequence of random variables $x_1$, $x_2$ . . . $x_T$ with transition kernel $q(x_t \\mid x_{t-1})$. The joint distribution of $x_1$, $x_2$ . . . $x_T$ conditioned on $x_0$, denoted as $q(x_1, . . . , x_T \\mid x_0)$: $$q(x_1, . . . , x_T \\mid x_0) = \\prod_{i=1}^T q(x_t \\mid x_{t-1}) \\tag{1.1}$$One typical design for the transition kernel is Gaussian perturbation, and the most common choice for the transition kernel is$$q(x_t \\mid x_{tâˆ’1}) = \\mathcal{N} (x_t ; \\sqrt{1 âˆ’ \\beta_t} x_{tâˆ’1} , \\beta_t I) \\tag{1.2}$$This Gaussian transition kernel allows us to marginalize the joint distribution in Eq. (1.1) to obtain the analytical form of $q(x_t \\mid x_0)$ for all $t \\in {0, 1, Â· Â· Â· ,T }$. Specifically, with $\\alpha_t := 1 âˆ’ Î²t$ and $\\overline\\alpha_t := \\prod_{s=0}^t \\alpha_s $, we have$$q(x_t \\mid x_0) = \\mathcal{N}(x_t; \\sqrt{\\overline\\alpha_t} x_0, (1-\\overline\\alpha_t)I) \\tag {1.3}$$Given $x_0$, we can easily obtain a sample of $x_t$ by sampling a Gaussian vector $\\epsilon \\sim \\mathcal{N} (0, I)$ and applying the transformation$$x_t = \\sqrt{\\overline\\alpha_t}x_0 + \\sqrt{1-\\overline\\alpha_t}\\epsilon \\tag{1.4}$$The learnable transition kernel $p_{\\theta} (x_{tâˆ’1} \\mid x_t )$ takes the form of$$p_\\theta(x_{t-1} \\mid x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)) \\tag{1.5}$$where $\\theta$ denotes model parameters, and the mean $ \\mu_\\theta(x_t , t)$ and variance $\\Sigma_\\theta(x_t , t)$ are parameterized by deep neural networks. Key to the success of this sampling process is training the reverse Markov chain to match the actual time reversal of the forward Markov chain. This is achieved by minimizing the Kullback-Leibler (KL) divergence. Ho et al. (2020) propose to reweight various terms in $L_{VLB}$ for better sample quality and noticed an important equivalence between the resulting loss function and the training objective for noise-conditional score networks (NCSNs), one type of score-based generative models, in Song and Ermon. The loss takes the form of$$\\mathbb{E}_{t\\sim\\mathcal{U}[1, T], x_0\\sim q(x_0), \\epsilon\\sim \\mathcal{N}(0, I)}\\left[\\lambda(t)\\Vert\\epsilon - \\epsilon_\\theta(x_t, t)\\Vert^2\\right] \\tag{1.6}$$where $\\lambda(t)$ is a positive weighting function, $x_t$ is computed from $x_0$ and $\\epsilon$ by Eq. (1.4), $\\mathcal{U}[1, T]$ s a uniform distribution over the set ${1, 2, Â· Â· Â· ,T }$, and $\\epsilon_\\theta$ is a deep neural network with parameter $\\theta$ that predicts the noise vector $\\epsilon$ given $x_t$ and $t$. Score-Based Generative Models (SGMs)The key idea of score-based generative models (SGMs) is to perturb data with a sequence of intensifying Gaussian noise and jointly estimate the score functions for all noisy data distributions by training a deep neural network model conditioned on noise levels (called a noise-conditional score network, NCSN). With similar notations in DDPMs, we let $q(x_0)$ be the data distribution, and $0 &lt; \\sigma_1 &lt; \\sigma_2 &lt; Â· Â· Â· &lt; \\sigma_t &lt; Â· Â· Â· &lt; \\sigma_T$ be a sequence of noise levels. A typical example of SGMs involves perturbing a data point $x_0$ to $x_t$ by the Gaussian noise distribution $q(x_t \\mid x_0) = \\mathcal{N} (x_t ; x_0, \\sigma_t^2 I )$. This yields a sequence of noisy data densities $q(x_1), q(x_2), Â· Â· Â· , q(x_T )$. With denoising score matching and similar notations in Eq. (1.6), the training objective is given by$$\\mathbb{E}_{t\\sim\\mathcal{U}\\text{ã€š}1, T \\text{ã€›}, x_0\\sim q(x_0), x_t\\sim q(x_t\\mid x_0)}\\left[\\lambda(t)\\sigma_t^2\\Vert \\nabla_{x_t}\\log {q(x_t)} - s_\\theta(x_t, t) \\Vert^2 \\right] \\tag{1.7}$$$$= \\mathbb{E}_{t\\sim\\mathcal{U}\\text{ã€š}1, T \\text{ã€›}, x_0\\sim q(x_0), x_t\\sim q(x_t\\mid x_0)}\\left[\\lambda(t)\\sigma_t^2\\Vert \\nabla_{x_t}\\log {q(x_t|x_0)} - s_\\theta(x_t, t) \\Vert^2 \\right] + const \\tag{1.8}$$$$= \\mathbb{E}_{t\\sim\\mathcal{U}\\text{ã€š}1, T \\text{ã€›}, x_0\\sim q(x_0), x_t\\sim q(x_t\\mid x_0)}\\left[\\lambda(t)\\Vert -\\frac{x_t - x_0}{\\sigma_t} - \\sigma_ts_\\theta(x_t, t) \\Vert^2 \\right] + const \\tag{1.9}$$$$= \\mathbb{E}_{t\\sim\\mathcal{U}\\text{ã€š}1, T \\text{ã€›}, x_0\\sim q(x_0), \\epsilon\\sim \\mathcal{N}(0, I)}\\left[\\lambda(t)\\Vert \\epsilon + \\sigma_ts_\\theta(x_t, t) \\Vert^2 \\right] + const \\tag{1.10}$$ Stochastic Differential Equations (Score SDEs)DDPMs and SGMs can be further generalized to the case of infinite time steps or noise levels, where the perturbation and denoising processes are solutions to stochastic differential equations (SDEs). We call this formulation Score SDE, as it leverages SDEs for noise perturbation and sample generation, and the denoising process requires estimating score functions of noisy data distributions. Score SDEs perturb data to noise with a diffusion process governed by the following stochastic differential equation (SDE):$$dx = f(x, t)dt + g(t)dw \\tag{1.11}$$where $f (x, t) $and $g(t)$ are diffusion and drift functions of the SDE, and w is a standard Wiener process (a.k.a., Brownian motion). The forward processes in DDPMs and SGMs are both discretizations of this SDE. As demonstrated in Song et al. (2020), for DDPMs, the corresponding SDE is:$$dx = -\\frac{1}{2}\\beta(t)xdt + \\sqrt{\\beta(t)}dw \\tag{1.12}$$where $\\beta(\\frac{t}{T}) = T\\beta_t$ as $T$ goes to infinity; and for SGMs, the corresponding SDE is given by$$dx = \\sqrt{\\frac{d[\\sigma(t)^2]}{dt}}dw \\tag{1.13}$$where $\\sigma(\\frac{t}{T}) = \\sigma_t$ as $T$ goes to infinity. Here we use $q_t (x)$ to denote the distribution of $x_t$ in the forward process. Crucially, for any diffusion process in the form of Eq. (1.9), Anderson shows that it can be reversed by solving the following reverse-time SDE:$$dx = \\left[f(x, t) - g(t)^2\\nabla_x\\log{q_t(x)}\\right]dt+g(t)d\\overline{w} \\tag{1.14}$$where $\\overline{w}$ is a standard Wiener process when time flows backwards, and $dt$ denotes an infinitesimal negative time step. Moreover, Song et al. (2020) prove the existence of an ordinary differential equation (ODE), namely the probability flow ODE, whose trajectories have the same marginals as the reverse-time SDE. The probability flow ODE is given by:$$dx = \\left[ f(x, t) - \\frac{1}{2}g(t)^2\\nabla_x\\log{q_t(x)} \\right]dt \\tag{1.15}$$Both the reverse-time SDE and the probability flow ODE allow sampling from the same data distribution as their trajectories have the same marginals. Like in SGMs, we parameterize a time-dependent score model $s_Î¸ (x_t , t)$ to estimate the score function by generalizing the score matching objective to continuous time, leading to the following objective:$$\\mathbb{E}_{t\\sim\\mathcal{U}[0, T], x_0\\sim q(x_0), x_t\\sim q(x_t\\mid x_0)}\\left[\\lambda(t)\\Vert s_\\theta(x_t, t) - \\nabla_{x_t}\\log {q_{0t}(x_t\\mid x_0)\\Vert^2} \\right] \\tag{1.16}$$where $\\mathcal{U}[0, T]$ denotes the uniform distribution over $[0,T ]$. Subsequent research on diffusion models focuses on improving these classical approaches (DDPMs, SGMs, and Score SDEs) from three major directions: faster and more efficient sampling, more accurate likelihood and density estimation, and handling data with special structures (such as permutation invariance, manifold structures, and discrete data).","link":"/2024/09/24/Diffusion-Model-Survey/"},{"title":"Docker container configuration","text":"A guidance of configuration for Docker container. Install requirementsCheck Nvidia1nvidia-smi Install Docker123sudo apt-get updatesudo apt-get install docker.iodocker --version Install Nvidia Docker Toolkit12sudo apt-get install nvidia-docker2sudo systemctl restart docker Create Docker container support for GPUsLook up in nvidia/cuda Tags | Docker Hub1sudo docker pull nvidia/cuda:12.4.1-base-ubuntu22.04 Create container1docker run -it --gpus all --name my_gpu_container nvidia/cuda:12.4.1-base-ubuntu22.04 /bin/bash Environment configuration in container12apt-get update... OthersStop container1docker stop my_gpu_container Start container1docker start my_gpu_container Enter an active container1docker exec -it my_gpu_container /bin/bash Delete container1docker rm my_gpu_container","link":"/2024/10/01/Docker-container-configuration/"},{"title":"DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation","text":"In this work, we present a new approach for â€œpersonalizationâ€ of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. Introduction A new approach for personalizing large text-to-image diffusion models enables them to generate specific subjects in various contexts. While current models create diverse images from text, they struggle to replicate exact appearances. This method introduces a rare token identifier for each subject, fine-tunes the model with both images and text, and applies a class-specific prior preservation loss to prevent overfitting. This allows the model to synthesize photorealistic images of the subject while maintaining its key features across different scenes (Figure 2). MethodText-to-Image Diffusion ModelsDiffusion models are probabilistic generative models that are trained to learn a data distribution by the gradual denoising of a variable sampled from a Gaussian distribution. They are trained using a squared error loss to denoise a variably-noised image or latent code $z_t := \\alpha_tx + \\sigma_t\\epsilon$ as follows:$$\\mathbb{E}_{x, c ,\\epsilon, t}\\left[ w_t \\Vert \\hat{x}_\\theta(\\alpha_tx + \\sigma_t\\epsilon, c) - x \\Vert^2_2 \\right] \\tag{1}$$details about the loss function referring to Diffusion Models: A Comprehensive Survey of Methods and Applications - Breynald Shelter. Personalization of Text-to-Image ModelsOur first task is to implant the subject instance into the output domain of the model such that we can query the model for varied novel images of the subject. Designing Prompts for Few-Shot PersonalizationIn order to bypass the overhead of writing detailed image descriptions for a given image set we opt for a simpler approach and label all input images of the subject â€œa [identifier] [class noun]â€, where [identifier] is a unique identifier linked to the subject and [class noun] is a coarse class descriptor of the subject (e.g. cat, dog, watch, etc.). In essence, we seek to leverage the modelâ€™s prior of the specific class and entangle it with the embedding of our subjectâ€™s unique identifier so we can leverage the visual prior to generate new poses and articulations of the subject in different contexts. Rare-token IdentifiersOur approach is to find rare tokens in the vocabulary, and then invert these tokens into text space, in order to minimize the probability of the identifier having a strong prior. We perform a rare-token lookup in the vocabulary and obtain a sequence of rare token identifiers $f (\\hat{V})$, where f is a tokenizer; a function that maps character sequences to tokens and $\\hat{V}$ is the decoded text stemming from the tokens $f (\\hat{V})$. Class-specific Prior Preservation LossTo mitigate the two aforementioned issues during fine-tuning: Model slowly forgets how to generate subjects of the same class as the target subject; The possibility of reduced output diversity. we propose an autogenous class-specific prior preservation loss that encourages diversity and counters language drift. In essence, our method is to supervise the model with its own generated samples, in order for it to retain the prior once the few-shot fine-tuning begins. Specifically, we generate data $x_{pr} = \\hat{x}(z_{t1} , c_{pr})$ by using the ancestral sampler on the frozen pre-trained diffusion model with random initial noise $z_{t1} \\sim \\mathcal{N} (0, I)$ and conditioning vector $c_{pr} := \\Gamma(f (â€œa [class noun]â€))$. The loss becomes:$$\\mathbb{E}_{x, c ,\\epsilon, \\epsilonâ€™, t}\\left[ w_t \\Vert \\hat{x}_\\theta(\\alpha_tx + \\sigma_t\\epsilon, c) - x \\Vert^2_2 \\right] + \\left[ \\lambda w_{tâ€™} \\Vert \\hat{x}_\\theta(\\alpha_{tâ€™}x_{pr} + \\sigma_{tâ€™}\\epsilonâ€™, c_{pr}) - x_{pr} \\Vert^2_2 \\right] \\tag{2}$$where the second term is the prior-preservation term that supervises the model with its own generated images, and $\\lambda$ controls for the relative weight of this term. ExpreimentsComparisons with Textual InversionWe compare our results with Textual Inversion, the recent concurrent work of Gal et al., using the hyperparameters provided in their work. We find that this work is the only comparable work in the literature that is subjectdriven, text-guided and generates novel images. We generate images for DreamBooth using Imagen, DreamBooth using Stable Diffusion and Textual Inversion using Stable Diffusion. We compute DINO and CLIP-I subject fidelity metrics and the CLIP-T prompt fidelity metric. In Table 1 we show sizeable gaps in both subject and prompt fidelity metrics for DreamBooth over Textual Inversion. Further, we compare Textual Inversion (Stable Diffusion) and DreamBooth (Stable Diffusion) by conducting a user study. We average results using majority voting and present them in Table 2. We find an overwhelming preference for DreamBooth for both subject fidelity and prompt fidelity. Prior Preservation Loss AblationThe prior preservation loss seeks to combat language drift and preserve the prior. We compute a prior preservation metric (PRES) by computing the average pairwise DINO embeddings between generated images of random subjects of the prior class and real images of our specific subject. The higher this metric, the more similar random subjects of the class are to our specific subject, indicating collapse of the prior. Additionally, we compute a diversity metric (DIV) using the average LPIPS cosine similarity between generated images of same subject with same prompt. We observe that our model trained with PPL achieves higher diversity (with slightly diminished subject fidelity), which can also be observed qualitatively in Figure 5, where our model trained with PPL overfits less to the environment of the reference images and can generate the dog in more diverse poses and articulations. Class-Prior AblationWe finetune Imagen on a subset of our dataset subjects (5 subjects) with no class noun, a randomly sampled incorrect class noun, and the correct class noun. Subject fidelity results are shown in Table 4, with substantially higher subject fidelity for our proposed approach. ApplicationsRecontextualization Art Renditions &amp; Property Modification LimitationsWe illustrate some failure models of our method in Figure 8. The first is related to not being able to accurately generate the prompted context. Possible reasons are a weak prior for these contexts, or difficulty in generating both the subject and specified concept together due to low probability of co-occurrence in the training set. The second is context-appearance entanglement, where the appearance of the subject changes due to the prompted context, exemplified in Figure 8 with color changes of the backpack. Third, we also observe overfitting to the real images that happen when the prompt is similar to the original setting in which the subject was seen. Other limitations are that some subjects are easier to learn than others (e.g. dogs and cats). Occasionally, with subjects that are rarer, the model is unable to support as many subject variations. Finally, there is also variability in the fidelity of the subject and some generated images might contain hallucinated subject features, depending on the strength of the model prior, and the complexity of the semantic modification. References[1]: Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman. DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 22500-22510","link":"/2024/09/26/DreamBooth/"},{"title":"HuggingFace Downloader","text":"Code pieces for downloading models or datasets from Hugging Face. Install12pip install huggingface_hubhuggingface-cli --help downloader.py1234567891011121314151617181920import osfrom huggingface_hub import snapshot_downloadrepo_id_list=[ # samples &quot;openai/clip-vit-base-patch32&quot;]# downloadfor repo_id in repo_id_list: print(&quot;Start Downloading for repo {}&quot;.format(repo_id)) local_dir = snapshot_download(repo_id=repo_id, repo_type='model', local_dir=os.path.join(&quot;Models&quot;,repo_id) , local_dir_use_symlinks=False, cache_dir=&quot;.cache&quot;, resume_download=True, endpoint=&quot;https://hf-mirror.com&quot;, use_auth_token=&quot;hf_*************&quot;) print(f&quot;File downloaded to {local_dir}&quot;) Run1python downloader.py","link":"/2024/09/22/HuggingFace-Downloader/"},{"title":"Environment configuration for MagicDrive","text":"A guidance of environment configuration for MagicDrive. Environment SetupPrepare conda environment 12conda create -n newenv python==3.8conda activate newenv Clone MagicDrive reop 1git clone --recursive https://github.com/cure-lab/MagicDrive.git Install pytorch==1.10.2, torchvision==0.11.3, cuda==11.3 12pip install torchvision-0.11.3+cu113-cp38-cp38-linux_x86_64.whlpip install torch-1.10.2+cu113-cp38-cp38-linux_x86_64.whl Prepare mmcv for BEVFusion 1pip install https://download.openmmlab.com/mmcv/dist/cu113/torch1.10.0/mmcv_full-1.4.5-cp38-cp38-manylinux1_x86_64.whl Install other requirements 1pip install -r requirements/dev.txt Install diffusers 12cd third_party/diffuserspip install . Install BEVFusion 1234cd third_partygit clone https://github.com/mit-han-lab/bevfusion.gitcd bevfusionpython setup.py develop Pretrained WeightsThe training is based on stable-diffusion-v1-5. Put them at ${ROOT}/pretrained/ as follows: 123456{ROOT}/pretrained/stable-diffusion-v1-5/â”œâ”€â”€ text_encoderâ”œâ”€â”€ tokenizerâ”œâ”€â”€ unetâ”œâ”€â”€ vaeâ””â”€â”€ ...","link":"/2024/09/23/Environment-configuration-for-MagicDrive/"},{"title":"InstructPix2Pix: Learning to Follow Image Editing Instructions","text":"We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained modelsâ€”a language model (GPT-3) and a text-to-image model (Stable Diffusion)â€”to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per-example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions. Introductionwe propose an approach for generating a paired dataset that combines multiple large models pretrained on different modalities: a large language model (GPT-3) and a text-to-image model (Stable Diffusion). These two models capture complementary knowledge about language and images that can be combined to create paired training data for a task spanning both modalities. Our model directly performs the image edit in the forward pass, and does not require any additional example images, full descriptions of the input/output images, or per-example finetuning. MethodWe treat instruction-based image editing as a supervised learning problem: first, we generate a paired training dataset of text editing instructions and images before/after the edit; then, we train an image editing diffusion model on this generated dataset Generating a Multi-modal Training DatasetGenerating Instructions and Paired CaptionsWe first operate entirely in the text domain, where we leverage a large language model to take in image captions and produce editing instructions and the resulting text captions after the edit, as shown in Figure 2a. See Table 1a for examples of our written instructions and output captions. Using this data, we fine-tuned the GPT-3 Davinci model for a single epoch using the default training parameters. See Table 1b for example GPT-3 generated data. Our dataset is created by generating a large number of edits and output captions using this trained model, where the input captions are real image captions from LAION-Aesthetics (excluding samples with duplicate captions or duplicate image URLs). Generating Paired Images from Paired CaptionsOne challenge in turning a pair of captions into a pair of corresponding images is that text-to-image models provide no guarantees about image consistency, even under very minor changes of the conditioning prompt. We therefore use Prompt-to-Prompt, a recent method aimed at encouraging multiple generations from a text-to-image diffusion model to be similar. While this greatly helps assimilate generated images, different edits may require different amounts of change in image-space. We therefore generate 100 sample pairs of images per caption-pair, each with a random $p âˆ¼ \\mathcal{U}(0.1, 0.9)$, and filter these samples by using a CLIP-based metric. This metric measures the consistency of the change between the two images (in CLIP space) with the change between the two image captions. InstructPix2PixWe use our generated training data to train a conditional diffusion model that edits images from written instructions. We base our model on Stable Diffusion, a large-scale text-to-image latent diffusion model. Details about training referring to [1]. ResultsWe show instruction-based image editing results on a diverse set of real photographs and artwork, for many edit types and instruction wordings. LimitationsWhile our method is able to produce a wide variety of compelling edits to images, including style, medium, and other contextual changes, there still remain a number of limitations: Our model is limited by the visual quality of the generated dataset, and therefore by the diffusion model used to generate the imagery. Furthermore, our methodâ€™s ability to generalize to new edits and make correct associations between visual changes and text instructions is limited by the human-written instructions used to fine-tune GPT-3, by the ability of GPT-3 to create instructions and modify captions, and by the ability of Prompt-to-Prompt to modify generated images. Additionally, we find that performing many sequential edits sometimes causes accumulating artifacts. Furthermore, there are well-documented biases in the data and the pretrained models that our method is based upon. The edited images from our method may inherit these biases or introduce others. References[1]: Tim Brooks, Aleksander Holynski, Alexei A. Efros. InstructPix2Pix: Learning To Follow Image Editing Instructions. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 18392-18402","link":"/2024/09/27/InstructPix2Pix/"},{"title":"HuggingFace datasets","text":"A guidance of usage for Hugging Face datasets. InstallStart by installing ğŸ¤— Datasets: 1pip install datasets ğŸ¤— Datasets also support audio and image data formats: 12pip install datasets[audio]pip install datasets[vision] Load DatasetImport load_dataset: 1from datasets import load_dataset Load remote dataset: 1dataset = load_dataset(&quot;glue&quot;, &quot;mrpc&quot;, split=&quot;train&quot;) Load local CSV files: 123dataset1 = load_dataset('csv', data_files='data.csv')dataset2 = load_dataset('csv', data_files=['train.csv', 'test.csv'])dataset3 = load_dataset('csv', data_files='/path/to/directory/*.csv') Load local JSON files: 1dataset = load_dataset('json', data_files='data.json') Load local TXT files: 1dataset = load_dataset('text', data_files='data.txt') Load from diskThe dataset downloaded to the local machine via the ğŸ¤— datasets library can be loaded using the load_from_disk() function. This function allows you to load datasets that have been previously cached or downloaded locally, without the need to fetch them again from the ğŸ¤— database. 12345from datasets import load_from_disklocal_dataset = load_from_disk(&quot;./local_imdb_dataset&quot;)print(local_dataset)","link":"/2024/10/14/HuggingFace-datasets/"},{"title":"Inversion-based Style Transfer with Diffusion Models","text":"Our key idea is to learn the artistic style directly from a single painting and then guide the synthesis without providing complex textual descriptions. Specifically, we perceive style as a learnable textual description of a painting. We propose an inversion-based style transfer method (InST), which can efficiently and accurately learn the key information of an image, thus capturing and transferring the artistic style of a painting. IntroductionIn this paper, we propose a novel example-guided artistic image generation framework (i.e., inversion-based style transfer, InST) which related to style transfer and text-to-image synthesis. Given only a single input painting image, our method can learn and transfer its style to a natural image with a very simple text prompt. To achieve this goal, we need to obtain the representation of the image style, which refers to the set of attributes that appear in the high-level textual description of the image. We define the textual descriptions as â€œnew wordsâ€ that do not exist in the normal language and obtain the embeddings via inversion method. Specifically, we propose an efficient and accurate textual inversion based on the attention mechanism, which can quickly learn key features from an image, and a stochastic inversion to maintain the semantic of the content image. MethodAs shown in Figure 3, our method involves the pixel, latent, and textual spaces. During training, image $x$ is the same as image $y$. The image embedding of image $x$ is obtained by the CLIP image encoder and then sent to the attention-based inversion module. Textual InversionDetails about textual inversion referring to An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion. However, this optimization-based approach is inefficient, and accurate embeddings are difficult to obtain without overfitting with a single image as training data. We propose a learning method based on multi-layer cross attention. The image encoder $\\tau_Î¸$ projects y to an image embedding $\\tau_\\theta(y)$. The multi-layer cross attention starts with $v_0 = \\tau_\\theta(y)$. Then each layer implements $Attention$ as follows:$$Q_i = W_Q^{(i)} \\cdot v_i, K = W_K^{(i)} \\cdot \\tau_\\theta(y), V = W_V^{(i)} \\cdot \\tau_\\theta(y) \\tag{1}$$ $$v_{i+1} = Attention (Q_i, K, V) \\tag {2}$$ During training, the model is conditioned by the corresponding text embedding only. To avoid overfitting, we apply a dropout strategy in each cross-attention layer, which is set to 0.05. Our optimization goal can finally be defined as follows:$$\\hat{v} = \\mathop{\\arg\\min}_v \\mathbb{E}_{z, x, y, t}\\left[ \\Vert \\epsilon - \\epsilon_\\theta(z_t, t, MultiAtt(\\tau_\\theta(y))) \\Vert^2_2 \\right] \\tag{3}$$where $z âˆ¼ E(x)$, $\\epsilon âˆ¼ \\mathcal N (0, 1)$. $\\tau_Î¸$ and $\\epsilon_Î¸$ are fixed during training. In this manner, $\\hat v$ is efficiently optimized to the target area. Stochastic InversionWe divide pre-trained text-to-image diffusion model-based image representation into two parts: holistic representation and detail representation. Holistic representation involves text conditions, and the detail representation is controlled by random noise. We first add random noise to the content image and then use the denoising U-Net in the diffusion model to predict the noise in the image. The predicted noise is used as the initial input noise during generation to preserve the content. Specifically, for each image $z$, the stochastic inversion module takes the image latent code $z = E(y)$ as input. Then $z_t$, the noisy version of $z$, is set as computable parameters, and $\\epsilon_t$ is obtained as follows:$$\\hat{\\epsilon_t} = (z_{t-1} - \\mu_T(z_t, t))\\sigma_t \\tag{4}$$ ExperimentsComparison with Style Transfer Methods Comparison with Text-Guided Methods Ablation StudyStochastic inversion: The full model can maintain the content information and reduce the impact of the style imageâ€™s semantic. Hyper-parameter Strength: The larger the Strength, the stronger the influence of the style image on the generated result, vice versa, the generated image is closer to the content image. Attention module: The multi-layer attention helps the content of the generated image be better controlled by the input text conditions and improves editability. Dropout: By dropping the parameters of the latent embeddings, both the accuracy and the editability are improved. LimitationsAlthough our method can transfer typical colors to some extent, when a significant difference exists between the colors of the content and reference images, our method may fail to semantically transfer the color in a one-to-one correspondence. References[1]: Y. Zhang et al., â€œInversion-based Style Transfer with Diffusion Models,â€ 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Vancouver, BC, Canada, 2023, pp. 10146-10156, doi: 10.1109/CVPR52729.2023.00978.","link":"/2024/09/28/Inversion-based-Style-Transfer-with-Diffusion-Models/"},{"title":"MathJax fails to render in hexo icarus","text":"In the Hexo Icarus theme, when navigating between pages, MathJax doesnâ€™t render immediately; it only renders after refreshing the page. This issue occurs because MathJax scripts are not being re-initialized on page transitions, which are typically handled by pjax in themes with smooth page transitions. To fix this, you can force MathJax to re-render on every end of the pjax event by using the following approach: 123456// listen pjax:end event in pjax.jsdocument.addEventListener('pjax:end', function () { // MathJax is global, can be used directly MathJax.typesetPromise();});","link":"/2024/09/27/MathJax-fails-to-render-in-hexo-icarus/"},{"title":"å¹½é»˜è¯­ç¯‡ç”Ÿæˆä¸ç†è§£çš„ç¤¾ä¼šå¿ƒç†é˜é‡Š","text":"å¹½é»˜æ—¢æ˜¯ä¸€ç§æ™ºæ…§æ­¦å™¨ï¼Œåˆæ˜¯æ”¿æ²»æ–—äº‰çš„æœ‰åŠ›æ­¦å™¨ã€‚å¹½é»˜è¯­ç¯‡ç”Ÿæˆåæ˜ äº†å¹½é»˜ä¸»ä½“ä¼˜è¶Šã€æ•Œè§†å’Œè¿½æ±‚æƒåˆ©åœ°ä½çš„ç¤¾ä¼šå¿ƒç†ã€‚å¹½é»˜è¯­ç¯‡ç†è§£çš„ç¤¾ä¼šå¿ƒç†ç‰¹ç‚¹åœ¨äºå¿ƒç†çš„çªç„¶æ‰‘ç©ºå’Œæ„å¤–ç»“å±€çš„ç¬é—´é†’æ‚Ÿç»™å—ä¼—å¸¦æ¥çš„æˆå°±æ„Ÿå’Œæ»¡è¶³æ„Ÿã€‚å¹½é»˜æ•ˆæœå—åˆ°äººä»¬çš„å¿ƒå¢ƒã€è”æƒ³èƒ½åŠ›å’Œå‚ä¸æ„è¯†ç­‰æ¡ä»¶çš„åˆ¶çº¦ã€‚ å¹½é»˜è¯­ç¯‡ç”Ÿæˆä¸Šçš„ç¤¾ä¼šå¿ƒç†ç‰¹å¾å¹½é»˜åæ˜ äººçš„ä¼˜è¶Šå¿ƒç†äººä»¬ç»å¸¸å˜²ç¬‘åˆ«äººçš„å¼±ç‚¹å’Œä¸å¹¸ã€‚æ¯”å¦‚å†¬å¤©çœ‹åˆ°æœ‰äººåœ¨å¤§è¡—ä¸Šäººæ»‘å€’æ—¶æ—è§‚è€…ä¼šæœ‰ä¸åŒçš„ååº”ï¼Œä½†å½“æ‘”å€’çš„äººä»åœ°ä¸Šçˆ¬èµ·å¹¶ä¸å¥½æ„æ€åœ°æŠ–è½èº«ä¸Šçš„é›ªæ—¶ï¼Œå¤§å¤šæ•°æ—è§‚è€…éƒ½ä¼šç¬‘ä¸€ç¬‘ã€‚è¿™ç§ç¬‘å°±æ˜¯å¯¹åˆ«äººä¸å¹¸çš„å˜²ç¬‘ã€‚äººä»¬éƒ½ä¸å–œæ¬¢å—äººå˜²ç¬‘ï¼Œå› ä¸ºå—å˜²ç¬‘å°±æ˜¯å—è½»è§†ã€‚å®éªŒè¡¨æ˜ï¼Œäººä»¬å¯¹é‚£äº›è‡ªå·±ä¸å–œæ¬¢çš„äººå‘ç”Ÿæ„å¤–æƒ…å†µæ—¶æ›´å®¹æ˜“ç¬‘èµ·æ¥ï¼Œè¿™æ­£è¯´æ˜å¹½é»˜æ˜¯äººä¼˜è¶Šæ„Ÿå¿ƒç†çš„è¡¨ç°æ–¹å¼ã€‚éœ€è¦æŒ‡å‡ºçš„æ˜¯ï¼Œå¼€ç©ç¬‘çš„äººå’Œè¢«å˜²ç¬‘çš„äººå¯¹ç¬‘è¯å°¤å…¶æ˜¯ä¾®è¾±æ€§ç¬‘è¯çš„æ€åº¦æ˜¯æˆªç„¶ä¸åŒçš„ã€‚è¢«å˜²ç¬‘çš„äººä¸€èˆ¬æ˜¯ä¸ç¬‘çš„ã€‚è¿™ä¸€ç‚¹è¿›ä¸€æ­¥è¯æ˜äº†å¹½é»˜æ˜¯ä¸ºç¤¾ä¼šåœ°ä½è€Œæ–—äº‰çš„ç‰¹æ®Šæ­¦å™¨ã€‚ å¹½é»˜åæ˜ äººçš„æ•Œè§†å¿ƒç†æ ¹æ®å¿ƒç†åˆ†æç†è®ºï¼Œå¹½é»˜åŠå…¶æ‰€äº§ç”Ÿçš„ç¬‘æ˜¯ä¸ºæŸä¸€ç¾¤ä½“çš„æ•Œè§†è¡Œä¸ºæœåŠ¡çš„ï¼Œæ— è®ºæ˜¯å¦æŒ‡å‘å…·ä½“çš„å¯¹è±¡ï¼Œå¹½é»˜éƒ½å¸¦æœ‰æ•Œè§†æ€§ã€‚Albert Rappï¼ˆ1951ï¼‰å’Œä»–çš„æ”¯æŒè€…ä»¬è®¤ä¸ºï¼Œç¬‘æ˜¯æ†æ¨å’Œæ•Œè§†çš„äº§ç‰©ã€‚æ•Œè§†æ˜¯äººç”Ÿæ¥å°±æœ‰çš„ï¼Œå¦‚æœäººç”Ÿæ¥ä¸ä¼šæ•Œè§†ï¼Œå°±ä¸ä¼šäº§ç”Ÿç¬‘ï¼Œä¹Ÿå°±ä¸ä¼šéœ€è¦å¹½é»˜ã€‚ç°ä»£å¹½é»˜çš„æ‰€æœ‰ç±»å‹éƒ½ä¿ç•™äº†æ•Œè§†æ€§ç‰¹ç‚¹ã€‚åªæ˜¯ä¸€äº›ç±»å‹ä¸­è¡¨ç°æ˜æ˜¾ï¼Œå¦ä¸€äº›ç±»å‹ä¸­è¡¨ç°æ¯”è¾ƒå«è“„ã€‚ å¹½é»˜å¯¹äººç±»çš„é‡è¦æ€§è¿˜æœ‰ä¸€ç‚¹å¯ä»¥è¯æ˜ï¼Œé‚£å°±æ˜¯å¾ˆå°‘æœ‰äººæ‰¿è®¤è‡ªå·±æ²¡æœ‰å¹½é»˜æ„Ÿã€‚æ¯ä¸ªäººéƒ½å¯èƒ½æ‰¿è®¤è‡ªå·±è§†åŠ›ä¸å¥½ã€ä¸ä¼šæ¸¸æ³³ã€ä¸ä¼šå°„å‡»ç­‰ç¼ºé™·ï¼Œä½†éƒ½ä¸ä¼šè®¤ä¸ºè‡ªå·±æ²¡æœ‰å¹½é»˜æ„Ÿã€‚å¦‚æœæˆ‘ä»¬è®¤ä¸ºå‘¨å›´çš„æŸäººæ²¡æœ‰å¹½é»˜æ„Ÿï¼Œé‚£ä¼šè¢«è®¤ä¸ºæ˜¯å¯¹ä»–è‡´å‘½çš„è”‘è§†ã€‚æ‰€ä»¥ï¼Œå¹½é»˜æ„Ÿè¢«äººä»¬æœ¬èƒ½åœ°è®¤ä¸ºæ˜¯è‡ªå·±ç”Ÿæ´»ä¸­æä¸ºé‡è¦çš„ä¸œè¥¿ï¼Œæ˜¯è¿½æ±‚æƒåŠ›å’Œè·å–èµ„æºçš„èƒ½åŠ›çš„è¡¨ç°ã€‚ å¹½é»˜åæ˜ äººè¿½æ±‚æƒåŠ›å’Œç¤¾ä¼šåœ°ä½çš„å¿ƒç†æ ¹æ® R. Provine å¯¹åŒä¸€èŒä¸šã€ä¸åŒèµ„å†çš„äººç¾¤çš„è°ƒæŸ¥ç»“æœï¼ŒèŒä½é«˜çš„äººçˆ±å¼€ç©ç¬‘ï¼Œåœ¨åŒä¸€æ—¶é—´å†…è®²ç¬‘è¯çš„æ•°é‡å¹³å‡è¾¾åˆ° 7.5 ä¸ªï¼ŒèŒä½ä¸­ç­‰çš„äººè®²ç¬‘è¯çš„æ•°é‡å¹³å‡ä¸º 5.5 ä¸ªï¼Œè€ŒèŒä½ä½çš„äººè®²ç¬‘è¯çš„æ•°é‡ä»…ä¸º 0.7 ä¸ªï¼ˆè½¬å¼•è‡ª Ğš.Ğ“Ğ»Ğ¸Ğ½ĞºĞ° 2004ï¼š33ï¼34ï¼‰ã€‚R. Provine æŒ‡å‡ºï¼Œåœ¨ä¸€äº›ç­‰çº§åˆ¶åº¦åˆ†æ˜çš„å›½å®¶é‡Œè¿™ä¸€ç‚¹è¡¨ç°å¾—æ›´ä¸ºçªå‡ºã€‚è¿™è¯´æ˜å¹½é»˜æ„Ÿæ˜¯ç¤¾ä¼šåœ°ä½é«˜ä½çš„ä¸€ç§è¡¨ç°å½¢å¼ã€‚åœ¨ä¸€ä¸ªäº¤é™…ç¾¤ä½“å†…éƒ¨åœ°ä½è¶Šé«˜ï¼Œå¼€ç©ç¬‘çš„æƒåˆ©è¶Šå¤§ã€æœºä¼šè¶Šå¤šï¼Œåœ°ä½é«˜çš„äººå¯ä»¥å˜²ç¬‘æ¯”è‡ªå·±åœ°ä½ä½çš„äººï¼Œåä¹‹ç¤¾ä¼šè§„èŒƒåˆ™ä¸å…è®¸ã€‚åœ¨æ°‘ä¸»å›½å®¶é‡Œï¼Œå¹½é»˜æ˜¯äº‰å¤ºæƒåŠ›çš„è‡´å‘½æ­¦å™¨ã€‚ è®²ç¬‘è¯äºº å¬ç¬‘è¯äºº ç¬‘è¯æ•°é‡ è®²ç¬‘è¯äººçš„ç¬‘æ¯”ä¾‹ å¬ç¬‘è¯äººçš„ç¬‘æ¯”ä¾‹ ç”·æ€§ ç”·æ€§ 275 75.6% 60.0% å¥³æ€§ å¥³æ€§ 502 86.0% 49.8% ç”·æ€§ å¥³æ€§ 238 66.0% 71.0% å¥³æ€§ ç”·æ€§ 185 88.1% 38.9% ä¸Šè¿°è°ƒæŸ¥ç»“æœæ˜¾ç¤ºï¼ŒæƒåŠ›æ„è¯†å¼ºçš„ç”·æ€§å¯¹å¥³æ€§æ‰€è®²çš„ç¬‘è¯åæ˜ ä¸å¤ªæ•æ„Ÿï¼Œç¬‘çš„æ¯”ä¾‹ä»…ä¸º 38.9%ï¼Œè€Œå¥³æ€§æ€»ä½“ä¸Šç¬‘çš„é¢‘ç‡æ¯”ç”·æ€§é«˜ï¼Œè€Œä¸”å¯¹ç”·æ€§æ‰€è®²ç¬‘è¯æ¯”è‡ªå·±åŒæ€§æ‰€è®²ç¬‘è¯çš„ç¬‘ååº”æ¬¡æ•°é«˜å‡º 20 å¤šä¸ªç™¾åˆ†ç‚¹ã€‚ä»ä¸Šè¡¨ä¸­æˆ‘ä»¬è¿˜ä¼šå‘ç°ï¼Œè®²ç¬‘è¯çš„äººæ¯”å¬ç¬‘è¯çš„äººç¬‘å¾—æ›´é¢‘ç¹ã€‚ å¯¹æ­¤ç°è±¡ Ğ.Ğ”Ğ¼Ğ¸Ñ‚Ñ€Ğ¸ĞµĞ²(1996:67) ä½œäº†è¿™æ ·çš„è§£é‡Šï¼šä»»ä½•äººåœ¨å’Œåˆ«äººäº¤å¾€æ—¶ï¼Œé€šå¸¸éƒ½ä¼šåŠ›äº‰ä¿æŒè‡ªå·±çš„å½¢è±¡ï¼Œæ ‘ç«‹è‡ªå·±çš„å¨ä¿¡ã€‚åœ¨åˆ«äººé¢å‰æ ‘ç«‹å¨ä¿¡æ˜¯æ¯ä¸ªäººçš„ç²¾ç¥éœ€æ±‚ï¼Œè¿™ä¸€éœ€æ±‚ä¿ƒä½¿ä»–ç§¯æè¡¨ç°ã€‚ç¬‘è¯å¼•äººå‘ç¬‘ï¼Œç»™è®²ç¬‘è¯çš„äººå¸¦æ¥å¿«æ„Ÿã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè®²ç¬‘è¯çš„äººé€šè¿‡ç¬‘è¯ä½¿äººå‘ç¬‘æé«˜äº†è‡ªå·±åœ¨åˆ«äººé¢å‰çš„åœ°ä½ï¼Œæ¿€å‘äº†ä¼˜è¶Šæ„Ÿï¼Œç”šè‡³æ¯æ¬¡è®²åŒä¸€ä¸ªç¬‘è¯æ—¶ä»ç„¶æ„Ÿåˆ°è‡ªå·±é«˜äºåˆ«äººï¼Œæ¯”åˆ«äººä¼˜å…ˆç†è§£äº†ç¬‘è¯çš„å«æ„ã€‚è¿™è¯´æ˜äººåœ¨å…¶ä»–äººé¢å‰è§†å¹½é»˜ä¸ºæå‡è‡ªå·±ç¤¾ä¼šåœ°ä½çš„å·¥å…·ï¼Œæ˜¯å®ç°é«˜äºåˆ«äººçš„ä¼˜è¶Šæ„Ÿçš„æ–¹å¼å’Œæ‰‹æ®µã€‚é‚£ä¹ˆï¼Œå—ä¼—çš„ç¬‘æ˜¯å¦ä¹Ÿæ„å‘³ç€æé«˜ç¤¾ä¼šåœ°ä½çš„éœ€æ±‚ï¼Ÿæˆ‘ä»¬è®¤ä¸ºï¼Œå¹½é»˜ä¸ä»…èƒ½æå‡è®²ç¬‘è¯äººçš„ç¤¾ä¼šåœ°ä½ï¼ŒåŒæ—¶ä¹Ÿä½¿å—ä¼—å¾—åˆ°å¿ƒç†ä¸Šçš„æ»¡è¶³ã€‚ç¬‘çš„ä¼˜å…ˆååº”è¯æ˜è‡ªå·±æ¯”åˆ«äººèªæ˜ï¼Œæ¯”é‚£äº›è¿Ÿé’çš„äººä¼˜è¶Šã€‚å› æ­¤ï¼Œç¬‘ååº”ä¸äººçš„ç¤¾ä¼šåœ°ä½æå‡æ„¿æœ›ç›¸å…³ï¼Œè€Œå¹½é»˜æ˜¯å®ç°è¿™ç§æå‡çš„æ‰‹æ®µå’Œæ­¦å™¨ï¼Œæ˜¯äººè¿½æ±‚åœ°ä½å’ŒæƒåŠ›çš„å¿ƒç†è¡¨ç°ã€‚ å¹½é»˜è¯­ç¯‡ç†è§£ä¸Šçš„ç¤¾ä¼šå¿ƒç†ç‰¹ç‚¹å—ä¼—çš„ç¤¾ä¼šå¿ƒç†ç‰¹ç‚¹å¹½é»˜æ•ˆæœä¸ä»…å–å†³äºå¹½é»˜çš„è´¨é‡å’Œè®²ç¬‘è¯çš„æŠ€å·§ï¼Œæ›´å–å†³äºå—ä¼—å½“æ—¶çš„å¿ƒå¢ƒå’Œå¹½é»˜æ„å¤–ç»“å±€çš„ç¬é—´é†’æ‚Ÿç»™ä»–å¸¦æ¥çš„æ»¡è¶³ç¨‹åº¦ã€‚å¹½é»˜æ•ˆæœæ˜¯è¿™ä¸¤é¡¹ç›¸åŠ çš„æ€»å’Œã€‚å—ä¼—çš„å¿ƒå¢ƒå†³å®šå¹½é»˜çš„æ•ˆæœã€‚äººåœ¨å¿ƒæƒ…å¥½çš„æ—¶å€™æ„¿æ„ç¬‘ï¼Œä¹Ÿå®¹æ˜“é€—ç¬‘ã€‚å¿ƒå¢ƒä¹Ÿæ˜¯å¯¹å¹½é»˜æ„å¤–ç»“å±€é†’æ‚Ÿçš„å‰æå’ŒåŸºç¡€ã€‚å¦‚æœå—ä¼—å¿ƒæƒ…ä¸å¥½ï¼Œå†é«˜è´¨é‡çš„å¹½é»˜ä¹Ÿä¸ä¼šæ¿€èµ·ä»–çš„å…´è¶£ï¼Œæ›´è°ˆä¸ä¸Šå¯¹å¹½é»˜ç»“å±€çš„ç¬é—´é†’æ‚Ÿã€‚ æ­£å¦‚å‰æ–‡æ‰€è¿°ï¼Œå¹½é»˜çš„ç›®çš„æ˜¯è·å¾—æˆ–å·©å›ºè‡ªå·±åœ¨ç¤¾ä¼šä¸Šçš„æŸç§ä¼˜è¶Šæ„Ÿï¼Œä¹Ÿè¢«ç§°ä¸ºæ„å¤–èƒœåˆ©æ„Ÿã€‚å› æ­¤æˆ‘ä»¬è®¤ä¸ºï¼Œå¹½é»˜æ•ˆæœæ°æ°æ˜¯é€šè¿‡äººçš„ç¤¾ä¼šåœ°ä½çš„æå‡æ‰€å¸¦æ¥çš„æˆå°±æ„Ÿæ¥è¾¾åˆ°çš„ã€‚æˆå°±æ„Ÿæ¥è‡ªäºåˆ«äººçš„èµèµå’Œè®¤å¯ã€å¼‚æ€§çš„å…³æ³¨æˆ–å…¶ä»–äººä»¬æ‰€çœ‹é‡çš„ä¸œè¥¿ã€‚å¹½é»˜çš„çœŸæ­£ç›®çš„å°±æ˜¯è¦è·å¾—è¿™ç§æˆå°±æ„Ÿï¼Œå¹½é»˜çš„æ•ˆæœå°±åœ¨äºæ”¹å˜ç¤¾ä¼šåœ°ä½ã€‚å¹½é»˜çš„æœ€ä½³æ•ˆæœä¸åœ¨äºç¬‘è¯åˆ›ä½œçš„ç²¾æ¹›ï¼Œä¹Ÿä¸åœ¨äºè®²ç¬‘è¯çš„é«˜è¶…æŠ€è‰ºï¼Œè€Œåœ¨äºè®²ç¬‘è¯å’Œå¬ç¬‘è¯çš„äººåŒæ—¶éƒ½å¾—åˆ°é«˜äºæ—äººçš„ä¼˜è¶Šæ„Ÿï¼Œåœ¨äºä»–ä»¬åœ¨ç¤¾ä¼šã€é›†ä½“ã€å®¶åº­ä¸­åœ°ä½çš„æå‡ä»¥åŠå¯¹å±€åŠ¿çš„æ§åˆ¶æ‰€å¸¦æ¥çš„æ»¡è¶³æ„Ÿã€‚ åœ¨å¹½é»˜è¯­ç¯‡ç†è§£çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬çŸ¥é“äº†å®ƒçš„å¼€å¤´å’Œæƒ…èŠ‚ä¹‹åï¼Œå°±ä¼šçŒœæµ‹å®ƒçš„ç»“å±€ã€‚å¤§è„‘ä¸­å°±ä¼šå½¢æˆå„ç§å„æ ·çš„é¢„æµ‹ç»“æœï¼Œåœ¨æˆ‘ä»¬å¬çš„è¿‡ç¨‹ä¸­ä¸€ç›´åœ¨ç­›é€‰ç€é¢„æµ‹çš„ç­”æ¡ˆã€‚ç”±äºæ—¶é—´çŸ­ï¼Œå¤‡é€‰çš„ç»“æœåˆå¤šï¼Œå¤§è„‘æ¥ä¸åŠå¯¹æ•°é‡ä¼—å¤šçš„å¯èƒ½æ€§ç»“æœè¿›è¡Œè¯„ä»·ï¼Œä¹Ÿæ— æ³•å¿«é€Ÿç¡®å®šå“ªäº›ç»“æœæ›´æœ‰å¯èƒ½æˆä¸ºç­”æ¡ˆï¼Œå› æ­¤æœ€åå¤§é‡çš„é¢„æµ‹ç»“æœåŒ–æ•´ä¸ºé›¶ï¼Œå¤´è„‘ä¸­åªå‰©ä¸‹ä¸€ä¸ªå¯èƒ½çš„ç­”æ¡ˆã€‚è€Œå¹½é»˜çš„ç»“å±€å´å‡ºäººæ„æ–™ï¼Œä½¿å—ä¼—å¤´è„‘ä¸­å”¯ä¸€çš„ç­”æ¡ˆä¹Ÿå¾—åˆ°å¦å®šã€‚æ„å¤–ç»“å±€çš„æ•ˆæœåœ¨äºï¼Œå¹½é»˜ä¸­æ‰€å‘ç”Ÿçš„äº‹ä»¶æ²¡æœ‰è¿›å…¥åˆ°å—ä¼—å¤§è„‘ä¸­å½¢æˆçš„ç­”æ¡ˆç¾¤ï¼ˆĞš.Ğ“Ğ»Ğ¸Ğ½ĞºĞ° 2004ï¼š47ï¼48ï¼‰ï¼Œå½¢æˆäº†å¿ƒç†çš„çªç„¶æ‰‘ç©ºå’Œç¬é—´é†’æ‚Ÿï¼Œä½¿å¹½é»˜äº§ç”Ÿå‡ºäººæ„æ–™çš„æ•ˆæœã€‚ å—ä¼—çš„å¿ƒç†æ¡ä»¶å—ä¼—è½»æ¾çš„å¿ƒç†çŠ¶æ€å’Œç²¾ç¥å‡†å¤‡æ­£å¦‚å‰æ–‡æ‰€è¿°ï¼Œå—ä¼—çš„å¿ƒå¢ƒå†³å®šå¹½é»˜çš„æ•ˆæœã€‚å¹½é»˜è¦æƒ³ä½¿äººå‘ç¬‘ï¼Œè¾¾åˆ°æœ€ä½³å¹½é»˜æ•ˆæœéœ€è¦å—ä¼—å…·å¤‡ä¸€äº›å¿ƒç†å‡†å¤‡ã€‚å¼—æ´›ä¼Šå¾·ã€è¾¾å°”æ–‡ç­‰å­¦è€…è®¤ä¸ºï¼Œäººå‘ç¬‘çš„å‰æé¦–å…ˆæ˜¯äººè½»æ¾æ„‰æ‚¦çš„å¿ƒç†çŠ¶æ€å’Œæ¥å—å¹½é»˜çš„ç²¾ç¥å‡†å¤‡ï¼ˆĞš.Ğ“Ğ»Ğ¸Ğ½ĞºĞ° 2004ï¼š42ï¼‰ã€‚ å—ä¼—çš„å¿ƒç†è”æƒ³å’Œå‚ä¸æ„è¯†ä¸€èˆ¬æ¥è¯´ï¼Œå¹½é»˜çš„ç†è§£è¿‡ç¨‹éœ€è¦å—ä¼—å¯¹å¹½é»˜è¯­ç¯‡çš„ä¸»äººå…¬è¿›è¡Œè”æƒ³å’Œæƒ³è±¡ï¼Œå»ºç«‹è‡ªå·±å’Œä¸»äººå…¬çš„è”ç³»ï¼Œæ„å¿µä¸­ä½¿è‡ªå·±å‚ä¸åˆ°ä»–çš„è¡Œä¸ºä¸­å»ï¼Œè®¾èº«å¤„åœ°åœ°æƒ³è±¡å¹½é»˜æƒ…æ™¯ï¼Œæ‰èƒ½æ·±åˆ»ç†è§£å¹½é»˜çš„å«æ„ï¼Œå–å¾—å¹½é»˜çš„æ•ˆæœã€‚å—ä¼—æ„Ÿåˆ°è‡ªå·±é«˜äºå¹½é»˜ä¸­çš„ä¸»äººå…¬ï¼Œè·å¾—ä¼˜è¶Šæ„Ÿï¼ŒåŒæ—¶åœ¨æ½œæ„è¯†ä¸­ä¸é‚£äº›æ²¡æœ‰å‚ä¸è¿›æ¥ã€å¯¹å¹½é»˜å«æ„æ²¡æœ‰é†’æ‚Ÿçš„å…¶ä»–å—ä¼—ä½œæ¯”è¾ƒï¼Œæ„Ÿåˆ°è‡ªå·±é«˜äºä»–ä»¬ï¼Œä»è€Œå¾—åˆ°äº†æå¤§çš„æ»¡è¶³æ„Ÿã€‚å› æ­¤è”æƒ³å’Œä¸»åŠ¨å‚ä¸ä¹Ÿæ˜¯å¹½é»˜è¯­ç¯‡ç†è§£çš„é‡è¦å‰æã€‚ ç»“è¯­å¹½é»˜æ—¢æ˜¯ä¸€ç§æ— å½¢çš„æ™ºæ…§æ­¦å™¨ï¼Œåˆæ˜¯æ”¿æ²»æ–—äº‰çš„æœ‰åŠ›æ­¦å™¨ï¼›æ˜¯å¹½é»˜ä¸»ä½“åœ¨ç¾¤ä½“ä¸­æé«˜ç¤¾ä¼šåœ°ä½ã€å¨ä¿¡ã€å¼•äººæ³¨ç›®ã€è·å–æˆå°±æ„Ÿçš„æœ‰æ•ˆæ‰‹æ®µã€‚å¹½é»˜è¯­ç¯‡ç”Ÿæˆåæ˜ äº†å¹½é»˜ä¸»ä½“ä¼˜è¶Šã€æ•Œè§†å’Œè¿½æ±‚æƒåŠ›åœ°ä½çš„ç¤¾ä¼šå¿ƒç†ã€‚å¹½é»˜è¯­ç¯‡ç†è§£çš„ç¤¾ä¼šå¿ƒç†ç‰¹ç‚¹åœ¨äºå¿ƒç†çš„çªç„¶æ‰‘ç©ºå’Œæ„å¤–ç»“å±€çš„ç¬é—´é†’æ‚Ÿç»™å—ä¼—å¸¦æ¥çš„æˆå°±æ„Ÿå’Œæ»¡è¶³æ„Ÿã€‚å¹½é»˜æ•ˆæœå—åˆ°äººä»¬çš„å¿ƒå¢ƒã€è”æƒ³èƒ½åŠ›å’Œå‚ä¸æ„è¯†ç­‰æ¡ä»¶çš„åˆ¶çº¦ã€‚ å‚è€ƒæ–‡çŒ®[1]: ç‹é‡‘ç². (2006). å¹½é»˜è¯­ç¯‡ç”Ÿæˆä¸ç†è§£çš„ç¤¾ä¼šå¿ƒç†é˜é‡Š. é»‘é¾™æ±Ÿå¤§å­¦å¤–å›½è¯­è¨€æ–‡å­¦åšå£«åæµåŠ¨ç«™ . æ”¶ç¨¿æ—¥æœŸ: 2006-06-15. www.sinoss.net/uploadfile/2010/1130/6367.pdf.","link":"/2024/10/12/The-Psychological-Process-of-Humor-Discourse-Comprehension-and-Audience-Response/"}],"tags":[{"name":"Diffusion Model","slug":"Diffusion-Model","link":"/tags/Diffusion-Model/"},{"name":"Personalization","slug":"Personalization","link":"/tags/Personalization/"},{"name":"Survey","slug":"Survey","link":"/tags/Survey/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"HuggingFace","slug":"HuggingFace","link":"/tags/HuggingFace/"},{"name":"Driving Video Deneration","slug":"Driving-Video-Deneration","link":"/tags/Driving-Video-Deneration/"},{"name":"Bugs","slug":"Bugs","link":"/tags/Bugs/"},{"name":"Humor","slug":"Humor","link":"/tags/Humor/"}],"categories":[{"name":"Paper","slug":"Paper","link":"/categories/Paper/"},{"name":"Environment Configuration","slug":"Environment-Configuration","link":"/categories/Environment-Configuration/"},{"name":"Diffusion Model","slug":"Paper/Diffusion-Model","link":"/categories/Paper/Diffusion-Model/"},{"name":"Code Piece","slug":"Code-Piece","link":"/categories/Code-Piece/"},{"name":"Bugs","slug":"Bugs","link":"/categories/Bugs/"},{"name":"Humor","slug":"Paper/Humor","link":"/categories/Paper/Humor/"}],"pages":[]}